{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A primer into TD Learning Algorithms using the Open AI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning is commonly classified in: i) supervised learning, ii) unsupervised learning and iii) reinforcement learning. Reinforcement learning differs from the supervised and unsupervised learning as the data we are trying to extract a model from is: neither labeled I/O pairs (supervised) from which we want to extract a function approximator nor unlabeled items (i.e., feature vectors) on which we desire identifying a relationship or schema by computing a distance (or viceversa similarity) measure. RL algorithms are types of simulation-based learning algorithms since they commonly require an agent interacting with its environment in order to perceive (with its sensors) a more or less observable environment state, formulate a plan (i.e., a sequence of actions) and actuate them to steer the environment to a different state.\n",
    "\n",
    "![rl-agent](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Reinforcement_learning_diagram.svg/300px-Reinforcement_learning_diagram.svg.png)\n",
    "\n",
    "The basic agent representation is the Markov decision process (MDP), consisting in a set of fully-observable states (i.e., deterministically detected by the agent) connected each other by actions: i) taken by the agent in each state and ii) a stochastic model of the environment, which is probabilistically reacting to the agent. \n",
    "The agent acts based on a task, i.e., he strives for optimizing an objective function, which in the discrete sense can be intended as achieving a certain environment configuration or state. Consequently, the task can have either a continuous (in time terms) or episodic nature, i.e., once achieved certain terminal states the agent's task ends. \n",
    "The agent task is expressed by means of a reward function, returned by the environment after each action.\n",
    "\n",
    "In real life it is not only the environment reaction to be stochastic (i.e. the actuation) but also the perception (i.e., available sensors might yield errors or not be enought to observe certain phenomena), thus leading to more complex mathematical frameworks, such a partially-observable MDPs, or PO-MDPs. The framework can also be further complicated by adding multiple agents and other constraints, but this goes outside the scope of this notebook.\n",
    "\n",
    "The goal of RL algorithms is therefore to learn a policy, defined as: i) a utility value related to each environment state, ii) a state-action pair function mapping the pair to the expected utility of taking a certain action in a specific state. When learning a policy two main aspects need to be considered: i) in the action selection strategy the exploitaion-exploration balance between running the action that leads highest expected reward and randomly selecting a suboptimal one that might eventually lead to higher cumulative reward; ii) in the backing of the achieved reward the \"credit assignment problem\", for which in real world rewards are typically delayed and the agent has to distinguish which actions in the sequence actually lead to the reward, and which instead made it diverge from it. This is typically done by assigning to an action the reward received for the actions that followed, since it would not have been possible by taking a different path. A discount factor is normally employed to give higher weight to closer (to current state) actions in the sequence.\n",
    "\n",
    "Please check the following links for a more complete understanding: \n",
    "\n",
    "RL Introduction\n",
    "* S.Russel, P.Norwig. Artificial Intelligence: a modern approach. Pearson 2010 (3rd edition)\n",
    "* R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT press 1998\n",
    "* C. Szepesvari. Algorithms for Reinforcement Learning. Morgan & Claypool publishers\n",
    "\n",
    "Multiagent RL\n",
    "* N.Nisan, T.Roughgarden, E.Tardos, V.V.Vazirani. Algorithmic game theory. Cambridge press 2007 \n",
    "* G.Weiss (editor). Multi agent systems. MIT press 2013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Gym\n",
    "The OpenAI Gym (https://github.com/openai/gym) is an environment specifically made to model control tasks and allow for the comparison of RL algorithms. \n",
    "\n",
    "Let's install it:  \n",
    "`python2.7 -m pip install -U --user gym`\n",
    "\n",
    "We can also install specific modules, such as the Atari environments or just everything:  \n",
    "`python2.7 -m pip install -U --user gym[atari]`  \n",
    "`python2.7 -m pip install -U --user gym[all]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channels_last\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "from keras import backend as K\n",
    "print K.image_data_format()\n",
    "K.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â 1. Selecting the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's list all available environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EnvSpec(RoadRunner-ramDeterministic-v4), EnvSpec(Asteroids-ramDeterministic-v0), EnvSpec(Gopher-ramDeterministic-v4), EnvSpec(RoadRunner-ramDeterministic-v0), EnvSpec(KellyCoinflipGeneralized-v0), EnvSpec(Gopher-ramDeterministic-v0), EnvSpec(DoubleDunk-ramDeterministic-v4), EnvSpec(DoubleDunk-ramDeterministic-v0), EnvSpec(Robotank-ram-v0), EnvSpec(CartPole-v0), EnvSpec(CartPole-v1), EnvSpec(Asteroids-ramDeterministic-v4), EnvSpec(Pooyan-ram-v4), EnvSpec(Gopher-ram-v0), EnvSpec(Pooyan-ram-v0), EnvSpec(Gopher-ram-v4), EnvSpec(Pong-ramNoFrameskip-v0), EnvSpec(FrozenLake8x8-v0), EnvSpec(YarsRevenge-ramDeterministic-v4), EnvSpec(CarRacing-v0), EnvSpec(SpaceInvaders-ram-v0), EnvSpec(YarsRevenge-ramDeterministic-v0), EnvSpec(SpaceInvadersDeterministic-v0), EnvSpec(DoubleDunk-ram-v0), EnvSpec(SpaceInvadersDeterministic-v4), EnvSpec(DoubleDunk-ram-v4), EnvSpec(VideoPinballDeterministic-v0), EnvSpec(Centipede-v0), EnvSpec(Centipede-v4), EnvSpec(YarsRevenge-v4), EnvSpec(Pitfall-ramNoFrameskip-v0), EnvSpec(Phoenix-ram-v4), EnvSpec(SpaceInvaders-ram-v4), EnvSpec(Pitfall-ramNoFrameskip-v4), EnvSpec(Frostbite-ramNoFrameskip-v0), EnvSpec(QbertNoFrameskip-v0), EnvSpec(AmidarNoFrameskip-v4), EnvSpec(QbertNoFrameskip-v4), EnvSpec(SkiingNoFrameskip-v4), EnvSpec(SkiingNoFrameskip-v0), EnvSpec(HotterColder-v0), EnvSpec(YarsRevenge-v0), EnvSpec(Phoenix-ram-v0), EnvSpec(Berzerk-ramNoFrameskip-v4), EnvSpec(Berzerk-ramNoFrameskip-v0), EnvSpec(KungFuMaster-ramNoFrameskip-v0), EnvSpec(AirRaidNoFrameskip-v4), EnvSpec(AirRaidDeterministic-v0), EnvSpec(ChopperCommandDeterministic-v4), EnvSpec(AirRaidNoFrameskip-v0), EnvSpec(AirRaidDeterministic-v4), EnvSpec(ChopperCommandDeterministic-v0), EnvSpec(Asteroids-ram-v0), EnvSpec(Asteroids-ram-v4), EnvSpec(TutankhamDeterministic-v0), EnvSpec(KrullDeterministic-v0), EnvSpec(MsPacman-ramNoFrameskip-v0), EnvSpec(Atlantis-ramDeterministic-v0), EnvSpec(KrullDeterministic-v4), EnvSpec(MsPacman-ramNoFrameskip-v4), EnvSpec(Atlantis-ramDeterministic-v4), EnvSpec(PhoenixNoFrameskip-v4), EnvSpec(PitfallDeterministic-v0), EnvSpec(Solaris-ram-v0), EnvSpec(Solaris-ram-v4), EnvSpec(Solaris-ramNoFrameskip-v4), EnvSpec(FishingDerbyNoFrameskip-v4), EnvSpec(VentureDeterministic-v4), EnvSpec(FishingDerbyNoFrameskip-v0), EnvSpec(VentureDeterministic-v0), EnvSpec(PhoenixNoFrameskip-v0), EnvSpec(Zaxxon-ramDeterministic-v0), EnvSpec(Qbert-v4), EnvSpec(ReversedAddition-v0), EnvSpec(Qbert-v0), EnvSpec(Robotank-ram-v4), EnvSpec(Pitfall-v0), EnvSpec(Pitfall-v4), EnvSpec(BipedalWalkerHardcore-v2), EnvSpec(Venture-ram-v4), EnvSpec(Venture-ram-v0), EnvSpec(MontezumaRevenge-ramNoFrameskip-v4), EnvSpec(MontezumaRevenge-ramNoFrameskip-v0), EnvSpec(Phoenix-ramNoFrameskip-v0), EnvSpec(Go9x9-v0), EnvSpec(MountainCarContinuous-v0), EnvSpec(SemisuperPendulumNoise-v0), EnvSpec(Reacher-v1), EnvSpec(ChopperCommand-ramNoFrameskip-v0), EnvSpec(ChopperCommand-ramNoFrameskip-v4), EnvSpec(Taxi-v2), EnvSpec(ZaxxonDeterministic-v0), EnvSpec(Pong-v0), EnvSpec(Enduro-v4), EnvSpec(Tutankham-ramDeterministic-v4), EnvSpec(Enduro-v0), EnvSpec(Tutankham-ramDeterministic-v0), EnvSpec(Krull-ramNoFrameskip-v0), EnvSpec(Krull-ramNoFrameskip-v4), EnvSpec(Freeway-ramNoFrameskip-v4), EnvSpec(ElevatorAction-ramNoFrameskip-v0), EnvSpec(ElevatorAction-ramNoFrameskip-v4), EnvSpec(StarGunner-ramNoFrameskip-v0), EnvSpec(AsteroidsDeterministic-v4), EnvSpec(StarGunner-ramNoFrameskip-v4), EnvSpec(Breakout-ram-v0), EnvSpec(Skiing-ram-v4), EnvSpec(Breakout-ram-v4), EnvSpec(Freeway-ram-v4), EnvSpec(Bowling-v0), EnvSpec(BattleZoneDeterministic-v4), EnvSpec(Bowling-v4), EnvSpec(BattleZoneDeterministic-v0), EnvSpec(AirRaid-ramDeterministic-v4), EnvSpec(AirRaid-ramDeterministic-v0), EnvSpec(CentipedeNoFrameskip-v0), EnvSpec(Skiing-ram-v0), EnvSpec(CentipedeNoFrameskip-v4), EnvSpec(Freeway-ram-v0), EnvSpec(Riverraid-ramNoFrameskip-v4), EnvSpec(ConvergenceControl-v0), EnvSpec(Riverraid-ramNoFrameskip-v0), EnvSpec(Pooyan-v4), EnvSpec(TimePilot-ramDeterministic-v4), EnvSpec(ChopperCommand-v0), EnvSpec(Pooyan-v0), EnvSpec(ChopperCommand-v4), EnvSpec(BattleZoneNoFrameskip-v0), EnvSpec(Alien-ramNoFrameskip-v4), EnvSpec(PrivateEye-v0), EnvSpec(BattleZoneNoFrameskip-v4), EnvSpec(Alien-ramNoFrameskip-v0), EnvSpec(PrivateEye-v4), EnvSpec(TimePilot-ramDeterministic-v0), EnvSpec(UpNDown-ramDeterministic-v4), EnvSpec(LunarLanderContinuous-v2), EnvSpec(TutankhamDeterministic-v4), EnvSpec(UpNDown-ramDeterministic-v0), EnvSpec(Phoenix-ramNoFrameskip-v4), EnvSpec(Freeway-ramNoFrameskip-v0), EnvSpec(KungFuMaster-ram-v4), EnvSpec(Asterix-ram-v0), EnvSpec(Jamesbond-ramNoFrameskip-v4), EnvSpec(Asterix-ram-v4), EnvSpec(Jamesbond-ramNoFrameskip-v0), EnvSpec(JourneyEscape-v0), EnvSpec(CrazyClimberDeterministic-v4), EnvSpec(SpaceInvaders-ramDeterministic-v4), EnvSpec(BipedalWalker-v2), EnvSpec(CrazyClimberDeterministic-v0), EnvSpec(QbertDeterministic-v0), EnvSpec(QbertDeterministic-v4), EnvSpec(KungFuMaster-ram-v0), EnvSpec(SolarisDeterministic-v0), EnvSpec(SolarisDeterministic-v4), EnvSpec(YarsRevengeDeterministic-v0), EnvSpec(SpaceInvaders-ramDeterministic-v0), EnvSpec(YarsRevengeDeterministic-v4), EnvSpec(TwoRoundDeterministicReward-v0), EnvSpec(Bowling-ramNoFrameskip-v0), EnvSpec(Bowling-ramNoFrameskip-v4), EnvSpec(TimePilotNoFrameskip-v4), EnvSpec(AmidarNoFrameskip-v0), EnvSpec(Seaquest-v0), EnvSpec(TimePilotNoFrameskip-v0), EnvSpec(MsPacmanDeterministic-v0), EnvSpec(Pooyan-ramDeterministic-v4), EnvSpec(Pooyan-ramDeterministic-v0), EnvSpec(PhoenixDeterministic-v4), EnvSpec(PhoenixDeterministic-v0), EnvSpec(CrazyClimber-ram-v0), EnvSpec(Frostbite-ramNoFrameskip-v4), EnvSpec(PredictActionsCartpole-v0), EnvSpec(CrazyClimber-ram-v4), EnvSpec(MontezumaRevenge-ram-v4), EnvSpec(MsPacmanDeterministic-v4), EnvSpec(BeamRider-ramDeterministic-v4), EnvSpec(Centipede-ramNoFrameskip-v0), EnvSpec(BeamRider-ramDeterministic-v0), EnvSpec(Centipede-ramNoFrameskip-v4), EnvSpec(KungFuMaster-v0), EnvSpec(OffSwitchCartpole-v0), EnvSpec(KungFuMaster-v4), EnvSpec(Jamesbond-ramDeterministic-v4), EnvSpec(Tennis-ramDeterministic-v4), EnvSpec(BreakoutDeterministic-v4), EnvSpec(Jamesbond-ramDeterministic-v0), EnvSpec(Tennis-ramDeterministic-v0), EnvSpec(BreakoutDeterministic-v0), EnvSpec(IceHockey-v0), EnvSpec(IceHockey-v4), EnvSpec(Pitfall-ramDeterministic-v4), EnvSpec(Venture-ramDeterministic-v0), EnvSpec(RobotankDeterministic-v4), EnvSpec(PongDeterministic-v4), EnvSpec(Venture-ramDeterministic-v4), EnvSpec(RobotankDeterministic-v0), EnvSpec(PongDeterministic-v0), EnvSpec(Pong-ramDeterministic-v0), EnvSpec(SeaquestNoFrameskip-v0), EnvSpec(Pong-ramDeterministic-v4), EnvSpec(SeaquestNoFrameskip-v4), EnvSpec(NameThisGame-ramNoFrameskip-v0), EnvSpec(Berzerk-v4), EnvSpec(Berzerk-v0), EnvSpec(MontezumaRevenge-v4), EnvSpec(TwoRoundNondeterministicReward-v0), EnvSpec(JourneyEscape-ramDeterministic-v4), EnvSpec(MontezumaRevenge-v0), EnvSpec(JourneyEscape-ramDeterministic-v0), EnvSpec(AirRaid-ram-v0), EnvSpec(Carnival-ram-v0), EnvSpec(AirRaid-ram-v4), EnvSpec(Zaxxon-v0), EnvSpec(KellyCoinflip-v0), EnvSpec(Carnival-ram-v4), EnvSpec(YarsRevenge-ramNoFrameskip-v0), EnvSpec(BeamRiderDeterministic-v0), EnvSpec(YarsRevenge-ramNoFrameskip-v4), EnvSpec(BeamRiderDeterministic-v4), EnvSpec(JourneyEscape-v4), EnvSpec(Phoenix-ramDeterministic-v0), EnvSpec(Phoenix-ramDeterministic-v4), EnvSpec(Skiing-v4), EnvSpec(Skiing-v0), EnvSpec(Boxing-ramDeterministic-v4), EnvSpec(Boxing-ramDeterministic-v0), EnvSpec(YarsRevenge-ram-v0), EnvSpec(PrivateEye-ram-v4), EnvSpec(PrivateEye-ram-v0), EnvSpec(Centipede-ramDeterministic-v0), EnvSpec(JourneyEscapeNoFrameskip-v4), EnvSpec(Centipede-ramDeterministic-v4), EnvSpec(JourneyEscapeNoFrameskip-v0), EnvSpec(BattleZone-ramDeterministic-v0), EnvSpec(NameThisGameNoFrameskip-v4), EnvSpec(Thrower-v0), EnvSpec(BattleZone-ramDeterministic-v4), EnvSpec(NameThisGameNoFrameskip-v0), EnvSpec(YarsRevenge-ram-v4), EnvSpec(Seaquest-ram-v0), EnvSpec(AsteroidsDeterministic-v0), EnvSpec(ElevatorAction-ram-v4), EnvSpec(Robotank-ramDeterministic-v0), EnvSpec(ElevatorAction-ram-v0), EnvSpec(ChopperCommand-ramDeterministic-v0), EnvSpec(Krull-ramDeterministic-v4), EnvSpec(ChopperCommand-ramDeterministic-v4), EnvSpec(Krull-ramDeterministic-v0), EnvSpec(BankHeistDeterministic-v0), EnvSpec(Frostbite-ram-v0), EnvSpec(BankHeistDeterministic-v4), EnvSpec(Frostbite-ram-v4), EnvSpec(Reverse-v0), EnvSpec(SeaquestDeterministic-v4), EnvSpec(Venture-ramNoFrameskip-v0), EnvSpec(SeaquestDeterministic-v0), EnvSpec(Robotank-ramDeterministic-v4), EnvSpec(JourneyEscape-ram-v0), EnvSpec(JourneyEscape-ram-v4), EnvSpec(VentureNoFrameskip-v4), EnvSpec(BerzerkDeterministic-v4), EnvSpec(VentureNoFrameskip-v0), EnvSpec(BerzerkDeterministic-v0), EnvSpec(AssaultNoFrameskip-v0), EnvSpec(Enduro-ramDeterministic-v0), EnvSpec(Tennis-ramNoFrameskip-v0), EnvSpec(Zaxxon-ramNoFrameskip-v4), EnvSpec(AssaultNoFrameskip-v4), EnvSpec(Enduro-ramDeterministic-v4), EnvSpec(Tennis-ramNoFrameskip-v4), EnvSpec(Gopher-ramNoFrameskip-v0), EnvSpec(IceHockey-ramDeterministic-v0), EnvSpec(Gopher-ramNoFrameskip-v4), EnvSpec(IceHockey-ramDeterministic-v4), EnvSpec(Humanoid-v1), EnvSpec(StarGunner-ramDeterministic-v0), EnvSpec(NameThisGame-ram-v4), EnvSpec(Tutankham-ramNoFrameskip-v4), EnvSpec(NameThisGame-ram-v0), EnvSpec(Tutankham-ramNoFrameskip-v0), EnvSpec(SemisuperPendulumRandom-v0), EnvSpec(Assault-ramDeterministic-v4), EnvSpec(Atlantis-ramNoFrameskip-v4), EnvSpec(EnduroDeterministic-v4), EnvSpec(Assault-ramDeterministic-v0), EnvSpec(Atlantis-ramNoFrameskip-v0), EnvSpec(EnduroDeterministic-v0), EnvSpec(BreakoutNoFrameskip-v0), EnvSpec(BreakoutNoFrameskip-v4), EnvSpec(OffSwitchCartpoleProb-v0), EnvSpec(NameThisGame-ramDeterministic-v4), EnvSpec(RoadRunner-ramNoFrameskip-v4), EnvSpec(NameThisGame-ramDeterministic-v0), EnvSpec(Frostbite-ramDeterministic-v4), EnvSpec(Frostbite-ramDeterministic-v0), EnvSpec(BankHeist-ramNoFrameskip-v4), EnvSpec(BankHeist-ramNoFrameskip-v0), EnvSpec(Qbert-ramNoFrameskip-v0), EnvSpec(Ant-v1), EnvSpec(RoadRunner-ramNoFrameskip-v0), EnvSpec(Qbert-ramNoFrameskip-v4), EnvSpec(Hero-ramNoFrameskip-v4), EnvSpec(Go19x19-v0), EnvSpec(Hero-ramNoFrameskip-v0), EnvSpec(Tennis-v0), EnvSpec(BeamRiderNoFrameskip-v4), EnvSpec(FishingDerby-ram-v0), EnvSpec(Enduro-ramNoFrameskip-v4), EnvSpec(FrostbiteNoFrameskip-v0), EnvSpec(BeamRiderNoFrameskip-v0), EnvSpec(FishingDerby-ram-v4), EnvSpec(Enduro-ramNoFrameskip-v0), EnvSpec(FrostbiteNoFrameskip-v4), EnvSpec(Gravitar-ramNoFrameskip-v4), EnvSpec(Skiing-ramNoFrameskip-v4), EnvSpec(CentipedeDeterministic-v4), EnvSpec(ReversedAddition3-v0), EnvSpec(Gravitar-ramNoFrameskip-v0), EnvSpec(Skiing-ramNoFrameskip-v0), EnvSpec(CentipedeDeterministic-v0), EnvSpec(Kangaroo-ram-v4), EnvSpec(Kangaroo-ram-v0), EnvSpec(VideoPinball-ramNoFrameskip-v0), EnvSpec(VideoPinball-ramNoFrameskip-v4), EnvSpec(Atlantis-ram-v4), EnvSpec(PongNoFrameskip-v4), EnvSpec(StarGunnerDeterministic-v0), EnvSpec(PongNoFrameskip-v0), EnvSpec(StarGunnerDeterministic-v4), EnvSpec(TimePilotDeterministic-v0), EnvSpec(CNNClassifierTraining-v0), EnvSpec(TimePilotDeterministic-v4), EnvSpec(Boxing-ram-v0), EnvSpec(Boxing-ram-v4), EnvSpec(Jamesbond-v4), EnvSpec(StarGunner-v0), EnvSpec(StarGunner-v4), EnvSpec(DemonAttackNoFrameskip-v0), EnvSpec(DemonAttackNoFrameskip-v4), EnvSpec(PooyanDeterministic-v0), EnvSpec(Amidar-ram-v4), EnvSpec(Jamesbond-v0), EnvSpec(Amidar-ram-v0), EnvSpec(ChopperCommandNoFrameskip-v0), EnvSpec(ChopperCommandNoFrameskip-v4), EnvSpec(Solaris-ramDeterministic-v4), EnvSpec(ElevatorActionDeterministic-v4), EnvSpec(Solaris-ramDeterministic-v0), EnvSpec(ElevatorActionDeterministic-v0), EnvSpec(Riverraid-ram-v0), EnvSpec(Riverraid-ram-v4), EnvSpec(Bowling-ramDeterministic-v0), EnvSpec(BattleZone-v0), EnvSpec(BattleZone-v4), EnvSpec(MsPacmanNoFrameskip-v0), EnvSpec(MsPacmanNoFrameskip-v4), EnvSpec(Bowling-ramDeterministic-v4), EnvSpec(Breakout-ramDeterministic-v0), EnvSpec(SolarisNoFrameskip-v0), EnvSpec(VideoPinballNoFrameskip-v0), EnvSpec(Breakout-ramDeterministic-v4), EnvSpec(SolarisNoFrameskip-v4), EnvSpec(VideoPinballNoFrameskip-v4), EnvSpec(PrivateEye-ramDeterministic-v0), EnvSpec(WizardOfWor-ram-v0), EnvSpec(PrivateEye-ramDeterministic-v4), EnvSpec(WizardOfWor-ram-v4), EnvSpec(Gravitar-v4), EnvSpec(RoadRunner-v4), EnvSpec(Gravitar-v0), EnvSpec(RoadRunner-v0), EnvSpec(MsPacman-ram-v4), EnvSpec(RoadRunner-ram-v0), EnvSpec(MsPacman-ram-v0), EnvSpec(RoadRunner-ram-v4), EnvSpec(Riverraid-ramDeterministic-v0), EnvSpec(Jamesbond-ram-v0), EnvSpec(Riverraid-ramDeterministic-v4), EnvSpec(VideoPinball-ram-v0), EnvSpec(UpNDownNoFrameskip-v0), EnvSpec(VideoPinball-ram-v4), EnvSpec(OneRoundDeterministicReward-v0), EnvSpec(FreewayNoFrameskip-v4), EnvSpec(WizardOfWorNoFrameskip-v0), EnvSpec(FreewayNoFrameskip-v0), EnvSpec(WizardOfWorNoFrameskip-v4), EnvSpec(Asterix-ramNoFrameskip-v0), EnvSpec(AlienNoFrameskip-v4), EnvSpec(Jamesbond-ram-v4), EnvSpec(Asterix-ramNoFrameskip-v4), EnvSpec(AlienNoFrameskip-v0), EnvSpec(BankHeist-ramDeterministic-v4), EnvSpec(UpNDownNoFrameskip-v4), EnvSpec(BankHeist-ramDeterministic-v0), EnvSpec(InvertedDoublePendulum-v1), EnvSpec(AsteroidsNoFrameskip-v4), EnvSpec(TimePilot-v4), EnvSpec(Asterix-v0), EnvSpec(AsteroidsNoFrameskip-v0), EnvSpec(TimePilot-v0), EnvSpec(Asterix-v4), EnvSpec(HeroDeterministic-v4), EnvSpec(ZaxxonDeterministic-v4), EnvSpec(HeroDeterministic-v0), EnvSpec(RiverraidNoFrameskip-v4), EnvSpec(WizardOfWor-ramNoFrameskip-v4), EnvSpec(ChopperCommand-ram-v4), EnvSpec(ChopperCommand-ram-v0), EnvSpec(SpaceInvaders-ramNoFrameskip-v0), EnvSpec(BowlingDeterministic-v4), EnvSpec(JamesbondDeterministic-v0), EnvSpec(SpaceInvaders-ramNoFrameskip-v4), EnvSpec(BowlingDeterministic-v0), EnvSpec(BankHeist-v0), EnvSpec(WizardOfWor-ramDeterministic-v0), EnvSpec(BankHeist-v4), EnvSpec(MsPacman-ramDeterministic-v4), EnvSpec(RiverraidNoFrameskip-v0), EnvSpec(NChain-v0), EnvSpec(FishingDerby-ramDeterministic-v0), EnvSpec(PredictObsCartpole-v0), EnvSpec(FishingDerby-ramDeterministic-v4), EnvSpec(CrazyClimber-v4), EnvSpec(CrazyClimber-ramNoFrameskip-v0), EnvSpec(JamesbondDeterministic-v4), EnvSpec(CrazyClimber-v0), EnvSpec(CrazyClimber-ramNoFrameskip-v4), EnvSpec(Seaquest-v4), EnvSpec(Enduro-ram-v4), EnvSpec(Pitfall-ramDeterministic-v0), EnvSpec(Enduro-ram-v0), EnvSpec(MsPacman-ramDeterministic-v0), EnvSpec(StarGunnerNoFrameskip-v4), EnvSpec(GravitarDeterministic-v4), EnvSpec(GravitarDeterministic-v0), EnvSpec(Swimmer-v1), EnvSpec(GravitarNoFrameskip-v4), EnvSpec(Alien-ram-v0), EnvSpec(Breakout-ramNoFrameskip-v0), EnvSpec(GravitarNoFrameskip-v0), EnvSpec(AsterixDeterministic-v4), EnvSpec(AsterixDeterministic-v0), EnvSpec(AlienDeterministic-v0), EnvSpec(AlienDeterministic-v4), EnvSpec(RoadRunnerDeterministic-v0), EnvSpec(Breakout-ramNoFrameskip-v4), EnvSpec(RoadRunnerDeterministic-v4), EnvSpec(RepeatCopy-v0), EnvSpec(FrostbiteDeterministic-v0), EnvSpec(TennisNoFrameskip-v4), EnvSpec(Carnival-ramDeterministic-v4), EnvSpec(EnduroNoFrameskip-v4), EnvSpec(KungFuMasterNoFrameskip-v0), EnvSpec(Carnival-ramDeterministic-v0), EnvSpec(EnduroNoFrameskip-v0), EnvSpec(KungFuMasterNoFrameskip-v4), EnvSpec(Tennis-v4), EnvSpec(Asteroids-ramNoFrameskip-v4), EnvSpec(Asteroids-ramNoFrameskip-v0), EnvSpec(TennisNoFrameskip-v0), EnvSpec(FrostbiteDeterministic-v4), EnvSpec(DemonAttackDeterministic-v0), EnvSpec(DemonAttackDeterministic-v4), EnvSpec(UpNDown-v4), EnvSpec(BankHeistNoFrameskip-v4), EnvSpec(DemonAttack-ram-v0), EnvSpec(UpNDown-v0), EnvSpec(BankHeistNoFrameskip-v0), EnvSpec(DemonAttack-ram-v4), EnvSpec(PooyanDeterministic-v4), EnvSpec(Kangaroo-ramDeterministic-v0), EnvSpec(Kangaroo-ramDeterministic-v4), EnvSpec(SemisuperPendulumDecay-v0), EnvSpec(WizardOfWor-ramDeterministic-v4), EnvSpec(RobotankNoFrameskip-v4), EnvSpec(Assault-ram-v4), EnvSpec(TutankhamNoFrameskip-v4), EnvSpec(RobotankNoFrameskip-v0), EnvSpec(WizardOfWor-v0), EnvSpec(Hopper-v1), EnvSpec(WizardOfWor-v4), EnvSpec(Pooyan-ramNoFrameskip-v4), EnvSpec(BattleZone-ramNoFrameskip-v0), EnvSpec(Pooyan-ramNoFrameskip-v0), EnvSpec(Asterix-ramDeterministic-v4), EnvSpec(ElevatorActionNoFrameskip-v0), EnvSpec(FrozenLake-v0), EnvSpec(ElevatorActionNoFrameskip-v4), EnvSpec(TutankhamNoFrameskip-v0), EnvSpec(JamesbondNoFrameskip-v4), EnvSpec(Hero-ramDeterministic-v0), EnvSpec(JamesbondNoFrameskip-v0), EnvSpec(Hero-ramDeterministic-v4), EnvSpec(HumanoidStandup-v1), EnvSpec(KungFuMaster-ramDeterministic-v4), EnvSpec(KungFuMaster-ramDeterministic-v0), EnvSpec(Amidar-v0), EnvSpec(Amidar-v4), EnvSpec(BerzerkNoFrameskip-v4), EnvSpec(BerzerkNoFrameskip-v0), EnvSpec(Amidar-ramNoFrameskip-v0), EnvSpec(Amidar-ramNoFrameskip-v4), EnvSpec(Striker-v0), EnvSpec(Gravitar-ramDeterministic-v0), EnvSpec(Asterix-ramDeterministic-v0), EnvSpec(Gravitar-ramDeterministic-v4), EnvSpec(BattleZone-ram-v4), EnvSpec(BattleZone-ram-v0), EnvSpec(IceHockey-ram-v0), EnvSpec(IceHockey-ram-v4), EnvSpec(BattleZone-ramNoFrameskip-v4), EnvSpec(MountainCar-v0), EnvSpec(BeamRider-ramNoFrameskip-v4), EnvSpec(Qbert-ramDeterministic-v0), EnvSpec(IceHockeyNoFrameskip-v4), EnvSpec(Carnival-v0), EnvSpec(IceHockeyNoFrameskip-v0), EnvSpec(Carnival-v4), EnvSpec(NameThisGameDeterministic-v0), EnvSpec(BeamRider-ramNoFrameskip-v0), EnvSpec(NameThisGameDeterministic-v4), EnvSpec(DoubleDunk-ramNoFrameskip-v0), EnvSpec(Solaris-v4), EnvSpec(IceHockey-ramNoFrameskip-v4), EnvSpec(Frostbite-v4), EnvSpec(DoubleDunk-ramNoFrameskip-v4), EnvSpec(IceHockey-ramNoFrameskip-v0), EnvSpec(MsPacman-v0), EnvSpec(Solaris-ramNoFrameskip-v0), EnvSpec(Gopher-v4), EnvSpec(MsPacman-v4), EnvSpec(Walker2d-v1), EnvSpec(Gopher-v0), EnvSpec(Hero-ram-v0), EnvSpec(Zaxxon-ram-v4), EnvSpec(Hero-ram-v4), EnvSpec(Zaxxon-ram-v0), EnvSpec(DoubleDunkDeterministic-v0), EnvSpec(Solaris-v0), EnvSpec(PooyanNoFrameskip-v4), EnvSpec(Frostbite-v0), EnvSpec(DoubleDunkDeterministic-v4), EnvSpec(Pong-v4), EnvSpec(PooyanNoFrameskip-v0), EnvSpec(Seaquest-ramNoFrameskip-v0), EnvSpec(FreewayDeterministic-v4), EnvSpec(Seaquest-ramNoFrameskip-v4), EnvSpec(FreewayDeterministic-v0), EnvSpec(Zaxxon-ramDeterministic-v4), EnvSpec(Blackjack-v0), EnvSpec(TennisDeterministic-v0), EnvSpec(TennisDeterministic-v4), EnvSpec(Atlantis-v4), EnvSpec(Assault-ram-v0), EnvSpec(Atlantis-v0), EnvSpec(GuessingGame-v0), EnvSpec(Copy-v0), EnvSpec(UpNDownDeterministic-v0), EnvSpec(CrazyClimber-ramDeterministic-v0), EnvSpec(Phoenix-v4), EnvSpec(CrazyClimber-ramDeterministic-v4), EnvSpec(Phoenix-v0), EnvSpec(PitfallDeterministic-v4), EnvSpec(StarGunner-ram-v0), EnvSpec(Asteroids-v0), EnvSpec(CarnivalDeterministic-v0), EnvSpec(StarGunner-ram-v4), EnvSpec(Asteroids-v4), EnvSpec(CarnivalDeterministic-v4), EnvSpec(IceHockeyDeterministic-v4), EnvSpec(OneRoundNondeterministicReward-v0), EnvSpec(IceHockeyDeterministic-v0), EnvSpec(FishingDerby-ramNoFrameskip-v4), EnvSpec(Qbert-ramDeterministic-v4), EnvSpec(Centipede-ram-v4), EnvSpec(PrivateEyeDeterministic-v4), EnvSpec(FishingDerby-ramNoFrameskip-v0), EnvSpec(PrivateEyeDeterministic-v0), EnvSpec(Centipede-ram-v0), EnvSpec(HeroNoFrameskip-v0), EnvSpec(Tennis-ram-v0), EnvSpec(Assault-v0), EnvSpec(Tennis-ram-v4), EnvSpec(Assault-v4), EnvSpec(UpNDownDeterministic-v4), EnvSpec(VideoPinball-ramDeterministic-v4), EnvSpec(HalfCheetah-v1), EnvSpec(PrivateEye-ramNoFrameskip-v4), EnvSpec(WizardOfWor-ramNoFrameskip-v0), EnvSpec(WizardOfWorDeterministic-v4), EnvSpec(GopherNoFrameskip-v0), EnvSpec(WizardOfWorDeterministic-v0), EnvSpec(GopherNoFrameskip-v4), EnvSpec(DoubleDunk-v4), EnvSpec(DoubleDunk-v0), EnvSpec(Tutankham-v0), EnvSpec(LunarLander-v2), EnvSpec(BeamRider-v4), EnvSpec(Tutankham-v4), EnvSpec(BeamRider-v0), EnvSpec(CarnivalNoFrameskip-v0), EnvSpec(BoxingDeterministic-v0), EnvSpec(TimePilot-ram-v0), EnvSpec(CarnivalNoFrameskip-v4), EnvSpec(BoxingDeterministic-v4), EnvSpec(Hex9x9-v0), EnvSpec(Alien-v0), EnvSpec(Berzerk-ram-v4), EnvSpec(Alien-v4), EnvSpec(Berzerk-ram-v0), EnvSpec(Robotank-v4), EnvSpec(AmidarDeterministic-v4), EnvSpec(Robotank-v0), EnvSpec(AmidarDeterministic-v0), EnvSpec(TimePilot-ram-v4), EnvSpec(Gravitar-ram-v0), EnvSpec(Venture-v4), EnvSpec(AssaultDeterministic-v0), EnvSpec(Gravitar-ram-v4), EnvSpec(Venture-v0), EnvSpec(AssaultDeterministic-v4), EnvSpec(Acrobot-v1), EnvSpec(Boxing-v0), EnvSpec(Boxing-v4), EnvSpec(ZaxxonNoFrameskip-v0), EnvSpec(DemonAttack-v0), EnvSpec(ZaxxonNoFrameskip-v4), EnvSpec(DemonAttack-v4), EnvSpec(Freeway-v0), EnvSpec(NameThisGame-v0), EnvSpec(Freeway-v4), EnvSpec(NameThisGame-v4), EnvSpec(RoadRunnerNoFrameskip-v4), EnvSpec(KungFuMasterDeterministic-v0), EnvSpec(RoadRunnerNoFrameskip-v0), EnvSpec(KungFuMasterDeterministic-v4), EnvSpec(Bowling-ram-v4), EnvSpec(Bowling-ram-v0), EnvSpec(Krull-ram-v0), EnvSpec(Seaquest-ramDeterministic-v0), EnvSpec(Krull-ram-v4), EnvSpec(Seaquest-ramDeterministic-v4), EnvSpec(TimePilot-ramNoFrameskip-v4), EnvSpec(TimePilot-ramNoFrameskip-v0), EnvSpec(BowlingNoFrameskip-v0), EnvSpec(BowlingNoFrameskip-v4), EnvSpec(VideoPinballDeterministic-v4), EnvSpec(UpNDown-ramNoFrameskip-v0), EnvSpec(UpNDown-ramNoFrameskip-v4), EnvSpec(Assault-ramNoFrameskip-v0), EnvSpec(Assault-ramNoFrameskip-v4), EnvSpec(CliffWalking-v0), EnvSpec(KungFuMaster-ramNoFrameskip-v4), EnvSpec(BankHeist-ram-v0), EnvSpec(PrivateEye-ramNoFrameskip-v0), EnvSpec(BankHeist-ram-v4), EnvSpec(Venture-ramNoFrameskip-v4), EnvSpec(YarsRevengeNoFrameskip-v0), EnvSpec(YarsRevengeNoFrameskip-v4), EnvSpec(DemonAttack-ramDeterministic-v4), EnvSpec(ElevatorAction-v0), EnvSpec(DemonAttack-ramDeterministic-v0), EnvSpec(ElevatorAction-v4), EnvSpec(FishingDerby-v4), EnvSpec(Carnival-ramNoFrameskip-v0), EnvSpec(FishingDerby-v0), EnvSpec(Carnival-ramNoFrameskip-v4), EnvSpec(MontezumaRevengeDeterministic-v4), EnvSpec(MontezumaRevengeDeterministic-v0), EnvSpec(PitfallNoFrameskip-v0), EnvSpec(DoubleDunkNoFrameskip-v0), EnvSpec(DoubleDunkNoFrameskip-v4), EnvSpec(AsterixNoFrameskip-v0), EnvSpec(AsterixNoFrameskip-v4), EnvSpec(PitfallNoFrameskip-v4), EnvSpec(StarGunner-ramDeterministic-v4), EnvSpec(HeroNoFrameskip-v4), EnvSpec(Amidar-ramDeterministic-v0), EnvSpec(Pong-ram-v4), EnvSpec(Amidar-ramDeterministic-v4), EnvSpec(Pong-ram-v0), EnvSpec(ElevatorAction-ramDeterministic-v0), EnvSpec(JourneyEscapeDeterministic-v0), EnvSpec(Pendulum-v0), EnvSpec(ElevatorAction-ramDeterministic-v4), EnvSpec(JourneyEscapeDeterministic-v4), EnvSpec(Breakout-v4), EnvSpec(Freeway-ramDeterministic-v0), EnvSpec(Breakout-v0), EnvSpec(Freeway-ramDeterministic-v4), EnvSpec(NameThisGame-ramNoFrameskip-v4), EnvSpec(Hero-v4), EnvSpec(Hero-v0), EnvSpec(InvertedPendulum-v1), EnvSpec(BeamRider-ram-v0), EnvSpec(Zaxxon-v4), EnvSpec(BeamRider-ram-v4), EnvSpec(VideoPinball-v0), EnvSpec(VideoPinball-v4), EnvSpec(PrivateEyeNoFrameskip-v0), EnvSpec(AtlantisDeterministic-v4), EnvSpec(PrivateEyeNoFrameskip-v4), EnvSpec(AtlantisDeterministic-v0), EnvSpec(Berzerk-ramDeterministic-v0), EnvSpec(UpNDown-ram-v4), EnvSpec(AirRaid-ramNoFrameskip-v4), EnvSpec(Berzerk-ramDeterministic-v4), EnvSpec(Seaquest-ram-v4), EnvSpec(AirRaid-ramNoFrameskip-v0), EnvSpec(MontezumaRevenge-ram-v0), EnvSpec(Roulette-v0), EnvSpec(MontezumaRevengeNoFrameskip-v0), EnvSpec(Riverraid-v4), EnvSpec(Atlantis-ram-v0), EnvSpec(Alien-ram-v4), EnvSpec(Boxing-ramNoFrameskip-v0), EnvSpec(Boxing-ramNoFrameskip-v4), EnvSpec(SpaceInvaders-v4), EnvSpec(Pitfall-ram-v4), EnvSpec(Skiing-ramDeterministic-v0), EnvSpec(RiverraidDeterministic-v4), EnvSpec(KangarooNoFrameskip-v4), EnvSpec(CrazyClimberNoFrameskip-v0), EnvSpec(AtlantisNoFrameskip-v4), EnvSpec(KangarooNoFrameskip-v0), EnvSpec(CrazyClimberNoFrameskip-v4), EnvSpec(AtlantisNoFrameskip-v0), EnvSpec(SpaceInvaders-v0), EnvSpec(Skiing-ramDeterministic-v4), EnvSpec(StarGunnerNoFrameskip-v0), EnvSpec(Kangaroo-ramNoFrameskip-v4), EnvSpec(RiverraidDeterministic-v0), EnvSpec(Kangaroo-ramNoFrameskip-v0), EnvSpec(SpaceInvadersNoFrameskip-v0), EnvSpec(DuplicatedInput-v0), EnvSpec(Robotank-ramNoFrameskip-v4), EnvSpec(Robotank-ramNoFrameskip-v0), EnvSpec(Zaxxon-ramNoFrameskip-v0), EnvSpec(SpaceInvadersNoFrameskip-v4), EnvSpec(DemonAttack-ramNoFrameskip-v0), EnvSpec(DemonAttack-ramNoFrameskip-v4), EnvSpec(GopherDeterministic-v0), EnvSpec(FishingDerbyDeterministic-v4), EnvSpec(Alien-ramDeterministic-v4), EnvSpec(GopherDeterministic-v4), EnvSpec(FishingDerbyDeterministic-v0), EnvSpec(Alien-ramDeterministic-v0), EnvSpec(VideoPinball-ramDeterministic-v0), EnvSpec(Qbert-ram-v0), EnvSpec(Tutankham-ram-v0), EnvSpec(Pong-ramNoFrameskip-v4), EnvSpec(Qbert-ram-v4), EnvSpec(Pitfall-ram-v0), EnvSpec(Tutankham-ram-v4), EnvSpec(AirRaid-v4), EnvSpec(KangarooDeterministic-v4), EnvSpec(AirRaid-v0), EnvSpec(KangarooDeterministic-v0), EnvSpec(JourneyEscape-ramNoFrameskip-v4), EnvSpec(Krull-v4), EnvSpec(JourneyEscape-ramNoFrameskip-v0), EnvSpec(Krull-v0), EnvSpec(Riverraid-v0), EnvSpec(KrullNoFrameskip-v0), EnvSpec(MontezumaRevengeNoFrameskip-v4), EnvSpec(KrullNoFrameskip-v4), EnvSpec(MontezumaRevenge-ramDeterministic-v0), EnvSpec(MontezumaRevenge-ramDeterministic-v4), EnvSpec(Pusher-v0), EnvSpec(BoxingNoFrameskip-v0), EnvSpec(SkiingDeterministic-v4), EnvSpec(BoxingNoFrameskip-v4), EnvSpec(SkiingDeterministic-v0), EnvSpec(UpNDown-ram-v0), EnvSpec(Kangaroo-v4), EnvSpec(Kangaroo-v0)]\n"
     ]
    }
   ],
   "source": [
    "from gym import envs\n",
    "print(envs.registry.all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start a standard environment the method make can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-11-20 14:40:53,781] Making new env: MsPacmanDeterministic-v4\n"
     ]
    }
   ],
   "source": [
    "env_id = \"MsPacmanDeterministic-v4\"\n",
    "env = gym.make(env_id)\n",
    "\n",
    "#env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__del__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_close', '_closed', '_elapsed_seconds', '_elapsed_steps', '_ensure_no_double_wrap', '_env_closer_id', '_episode_started_at', '_max_episode_seconds', '_max_episode_steps', '_owns_render', '_past_limit', '_render', '_reset', '_seed', '_spec', '_step', 'action_space', 'class_name', 'close', 'configure', 'env', 'metadata', 'observation_space', 'render', 'reset', 'reward_range', 'seed', 'spec', 'step', 'unwrapped']\n"
     ]
    }
   ],
   "source": [
    "print dir(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method close removes the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Running an example experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most important in every reproducible experiment is to set a seed for the random generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_seed = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a pacman environment and where we select random actions until the agent dies or clears the level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-11-20 14:40:57,849] Making new env: MsPacmanDeterministic-v4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space Discrete(9)\n",
      "actions ['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'UPRIGHT', 'UPLEFT', 'DOWNRIGHT', 'DOWNLEFT']\n",
      "observation space Box(210, 160, 3)\n",
      "The agent has died\n",
      "Total reward 150.0\n"
     ]
    }
   ],
   "source": [
    "# reset the environment to the initial state\n",
    "#env_id = \"MsPacman-v0\"\n",
    "env = gym.make(env_id)\n",
    "\n",
    "observation = env.reset()\n",
    "\n",
    "np.random.seed(initial_seed)\n",
    "env.seed(initial_seed)\n",
    "\n",
    "print \"action space\", env.action_space\n",
    "print \"actions\", env.env.get_action_meanings()\n",
    "print \"observation space\", env.observation_space\n",
    "\n",
    "done = False\n",
    "info = None\n",
    "total_reward = 0\n",
    "while (not done) and (info is None or info['ale.lives'] == 3):\n",
    "    # update environment\n",
    "    env.render()\n",
    "    # select random action from the action space\n",
    "    action = env.action_space.sample()\n",
    "    # apply action on the environment\n",
    "    # get back the observation of the destination state, the reward, if terminal state was reached\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    \n",
    "if info['ale.lives'] == 2:\n",
    "    print \"The agent has died\"\n",
    "print \"Total reward\", total_reward\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'render.modes': ['human', 'rgb_array']}\n",
      "['__class__', '__del__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_close', '_closed', '_elapsed_seconds', '_elapsed_steps', '_ensure_no_double_wrap', '_env_closer_id', '_episode_started_at', '_max_episode_seconds', '_max_episode_steps', '_owns_render', '_past_limit', '_render', '_reset', '_seed', '_spec', '_step', 'action_space', 'class_name', 'close', 'configure', 'env', 'metadata', 'observation_space', 'render', 'reset', 'reward_range', 'seed', 'spec', 'step', 'unwrapped']\n"
     ]
    }
   ],
   "source": [
    "print env.metadata\n",
    "print dir(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Selecting a Reinforcement Learning library on the top of Keras\n",
    "\n",
    "Looking for what libraries can seamlessly integrate with Tensorflow & Keras to provide RL functionalities I have found the following:\n",
    "* RLlab (for OpenAI Gym) - https://github.com/rll/rllab, now renamed Garage - https://github.com/rlworkgroup/garage\n",
    "* TRFL (Truffle) - https://github.com/deepmind/trfl/\n",
    "* Tensorforce - https://github.com/reinforceio/tensorforce\n",
    "* Keras-RL - https://github.com/keras-rl/keras-rl\n",
    "\n",
    "The first 3 are based on Tensorflow, while only Keras-RL is based on Keras and well integrated with OpenAI Gym.  \n",
    "Let's install it:  \n",
    "`python2.7 -m pip install -U --user keras-rl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Implementing an RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Permute\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD, rmsprop\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.models import clone_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from rl.agents.cem import CEMAgent\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.memory import EpisodeParameterMemory, SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, agent_type, env_id, input_shape, window_length, num_actions, optimizer, metrics, \n",
    "                 preprocessor=None):\n",
    "        self.env_id = env_id\n",
    "        self.preprocessor = preprocessor\n",
    "        self.model = self.__get__model(input_shape, window_length, num_actions)\n",
    "        #memory = EpisodeParameterMemory(limit=1000,\n",
    "        #                                window_length=1)\n",
    "        memory = SequentialMemory(limit=1000000, window_length=window_length)\n",
    "        # limit: how many entries the memory can hold, after which older entries will be replaced with newer ones\n",
    "        # window_length: how many observations are concatenated to form a state\n",
    "        \n",
    "        self.agent = self.__get_decision__maker(self.model, num_actions, memory, agent_type=agent_type)\n",
    "        self.agent.compile(optimizer, metrics)\n",
    "        \n",
    "    def __get_decision__maker(self, model, num_actions, memory, agent_type=\"DQN\"):\n",
    "        if agent_type is \"DQN\":\n",
    "            #dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10, enable_dueling_network=True, dueling_type='avg', target_model_update=1e-2, policy=policy)\n",
    "            \n",
    "            policy = LinearAnnealedPolicy(EpsGreedyQPolicy(),\n",
    "                                          attr='eps',\n",
    "                                          value_max=1.,\n",
    "                                          value_min=.1, \n",
    "                                          value_test=.05,\n",
    "                                          nb_steps=1000000)\n",
    "            a = DQNAgent(model=model,\n",
    "                         processor=self.preprocessor,\n",
    "                         nb_actions=num_actions,\n",
    "                         memory=memory,\n",
    "                         nb_steps_warmup=2000,\n",
    "                         policy=policy,\n",
    "                         test_policy=None,\n",
    "                         enable_double_dqn=True,\n",
    "                         enable_dueling_network=False,\n",
    "                         dueling_type='avg')\n",
    "        else:\n",
    "            a = CEMAgent(model=model, \n",
    "                         processor=self.preprocessor,\n",
    "                         nb_actions=num_actions,\n",
    "                         memory=memory,\n",
    "                         batch_size=50,\n",
    "                         nb_steps_warmup=2000,\n",
    "                         train_interval=50,\n",
    "                         elite_frac=0.05)\n",
    "        return a\n",
    "    \n",
    "    def __get__model(self, input_shape, window_length, num_actions):\n",
    "        model = Sequential()\n",
    "        # ValueError: Error when checking : expected conv2d_82_input to have shape\n",
    "        # (None, 80, 88, 1) but got array with shape (1, 1, 88, 80)\n",
    "        # for some reason, keras RL is passing images transposed, i.e., with shape (batch, channel, h, w)\n",
    "        model.add(Permute((2, 3, 1),\n",
    "                          input_shape = (window_length,)+input_shape,\n",
    "                          #data_format = \"channels_last\", # without this the channel is provided first\n",
    "                         ))\n",
    "        model.add(Conv2D(32,\n",
    "                         (3, 3), \n",
    "                         #input_shape=(window_length,)+input_shape,\n",
    "                         #data_format = \"channels_last\", # without this the channel is provided first\n",
    "                         activation='relu'))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        #model.add(Flatten(input_shape=(1,) + num_sensors))\n",
    "        model.add(Flatten())\n",
    "        # add intermediate layers\n",
    "        model.add(Dense(512, activation=\"relu\"))\n",
    "        model.add(Dropout(0.35))\n",
    "        model.add(Dense(256, activation=\"relu\"))\n",
    "        model.add(Dropout(0.35))\n",
    "        model.add(Dense(128, activation=\"relu\"))\n",
    "        model.add(Dropout(0.35))\n",
    "        # decision layer: softmax\n",
    "        #model.add(Activation('softmax'))\n",
    "        model.add(Dense(num_actions, activation='softmax'))\n",
    "        return model\n",
    "    \n",
    "    def __print_model_summary__(self):\n",
    "        print self.model.summary()\n",
    "        \n",
    "    def __train__(self, env, steps=100000, visualize=False, verbose=2, overwrite=True):\n",
    "        self.agent.fit(env,\n",
    "                       nb_max_episode_steps=None, # the environment is episodic, undefinite number of steps\n",
    "                       nb_steps=steps, # number of training steps\n",
    "                       visualize=visualize,\n",
    "                       verbose=verbose)\n",
    "        # save the weights\n",
    "        self.agent.save_weights('weights_{}_params.h5f'.format(env_id), overwrite=overwrite)\n",
    "    \n",
    "    def __test__(self, env, episodes=5, visualize=True):\n",
    "        self.agent.test(env, nb_episodes=episodes, visualize=visualize)\n",
    "        \n",
    "    def __replay__(self, env, condition):\n",
    "        done = False\n",
    "        info = None\n",
    "        total_reward = 0\n",
    "        observation = env.reset()\n",
    "        while (not done) and condition:\n",
    "            # update environment\n",
    "            env.render()\n",
    "            # select random action from the action space\n",
    "            action = env.action_space.sample()\n",
    "            # apply action on the environment\n",
    "            # get back the observation of the destination state, the reward, if terminal state was reached\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "        print \"Total reward\", total_reward\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-11-20 18:03:59,179] Making new env: MsPacmanDeterministic-v4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_sensors (210, 160, 3)\n",
      "num_actions 9\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_id)\n",
    "initial_state = env.reset()\n",
    "state_shape = env.observation_space.shape\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "print \"num_sensors\", state_shape\n",
    "print \"num_actions\", num_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img    \n",
    "from rl.core import Processor\n",
    "from PIL import Image\n",
    "\n",
    "class ObservationProcessor(Processor):\n",
    "    def __init__(self, downsamples=1, resize_to=None):\n",
    "        self.downsamples=downsamples\n",
    "        # resize_to is passed in format (w,h)\n",
    "        self.resize_to=resize_to\n",
    "    \n",
    "    def process_observation(self, state_observation):\n",
    "        #print \"state_observation\", type(state_observation), state_observation.shape\n",
    "        # we should expect images of format (height, width, channel), i.e., (210, 160, 3)\n",
    "        assert state_observation.ndim == 3\n",
    "        # crop to the labirinth to remove bottom information\n",
    "        state_observation = state_observation[1:172, :]\n",
    "        # this below downsamples the number of pixels (e.g. half when 2)\n",
    "        state_observation = state_observation[::self.downsamples, ::self.downsamples]\n",
    "        state_observation = state_observation.mean(axis=2)\n",
    "        if self.resize_to is not None:\n",
    "            # convert the numpy array to a easily-resizable image\n",
    "            img_obs = Image.fromarray(state_observation)\n",
    "            # resize the image (w, h)\n",
    "            #https://pillow.readthedocs.io/en/3.1.x/reference/Image.html#PIL.Image.Image.resize\n",
    "            img_obs = img_obs.resize(self.resize_to[:2])\n",
    "            # reconvert to numpy array\n",
    "            state_observation = np.array(img_obs)\n",
    "            state_observation = state_observation.reshape(self.resize_to[0], self.resize_to[1]) #, self.resize_to[2])\n",
    "        state_observation = state_observation.astype('uint8')\n",
    "        #print \"preprocessed state shape\", state_observation.shape\n",
    "        return state_observation\n",
    "        #return state_observation.transpose()\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        #print \"process_state_batch_0\", batch.shape\n",
    "        #return batch.astype('float32') / 255.\n",
    "        # ValueError: Error when checking : expected conv2d_123_input to have shape (None, 80, 88, 1)\n",
    "        # but got array with shape (1, 1, 88, 80)\n",
    "        #batch = np.reshape(batch, (batch.shape[0], batch.shape[2], batch.shape[1], batch.shape[3]))\n",
    "        \"\"\"\n",
    "        tmp_b = []\n",
    "        for i in range(batch.shape[0]):\n",
    "            b_i_t = batch[i,0,:,:].transpose()\n",
    "            #print \"b_i_t.shape\", b_i_t.shape\n",
    "            b_i_t = np.reshape(b_i_t, (b_i_t.shape[0], b_i_t.shape[1], 1))\n",
    "            #print \"b_i_t.shape\", b_i_t.shape\n",
    "            tmp_b.append( b_i_t )\n",
    "        batch = np.array(tmp_b)\n",
    "        \"\"\"\n",
    "        #print \"process_state_batch_1\", batch.shape\n",
    "        batch = batch.astype('float32') / 255.\n",
    "        return batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11e5974d0>"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAM4AAAEACAYAAAD7ikm0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nq1SmyLYrXM8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e35f5d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(initial_state, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h=80\n",
    "w=80\n",
    "resized_state_shape = (w,h)\n",
    "\n",
    "op = ObservationProcessor(resize_to=resized_state_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 80)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11e7b3310>"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP0AAAD/CAYAAAA6/dD3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztvXtwW9md5/chwSdIiuATeoF6kK0HaVmk1Wmqe1pS21Jv\nadzdk2m3K72j7q0pJ7U1f+xmJ9nKZDybSsr+I+Wx/9lJpSpVqZ3Zqc1Y7Z3seNsZ2514JI1blK1u\n6gVKFCiRIvgA+AIBEAABAgRBgPkDvFe4IinicS9wQZ5PFYrk4cE993dwf7jnnnN+vy8IBAKBQCAQ\nCAQCgUAgEAgEAoFAIBAIBIIi5jLwFHgG/GmBz0UgEGiMARgFDgPlwABwspAnJBAItqc0h/e+RtLp\nJ4AY8B+B/1KFcxIIBBqSi9MfAJwpf0+tlwkEAh2Ti9OvbVfhwoULa+v1xEu8xCuPr3Xf25Syrf6R\nBtOAJeVvC8m7vczNmze5cOECb731FgCdnZ2MjY3xySefMDg4mEPTheeDDz7go48+4uHDh3zve98D\nwOl0cvXq1R1h307h9OnTXLlyhY8//pj9+/fL5T/96U+5evUqn376aQHPLnck+44ePcrjx4/l8u9/\n//tbvicXp78HvEJyIm8G+BD4gxcrvfXWWwqnGBsby6FJgUCwGW+88Qbf/va35b+1cvpV4F8CvyI5\nk/9XwJMcjicQCPJALk4P8P+uv7ZEGtrvVHa6fYKdRy4TeWmx051ip9sn2HnkeqdXhZqaGkwmEyaT\nidLS5PdQIpHA7/fj9/tZWlqS61ZVVcl1y8vL5fJAIIDf72dxcVEuKy8vl+tWVVXJ5aFQSD722lpy\nkrO0tFSuW1NTI9ddXl6W68ZisV1tn5bt1dfXYzKZ2LNnj1wWi8XkusvLy1n1ZybsdPskdOH0ZrOZ\nnp4euru7KStLnlIsFsNqtTIwMMD4+Lhct6Ghge7ubnp6eqirq5PLbTYbVqsVm80mlxmNRrq6uuju\n7sZsNsvlY2Nj8rFXV1cBMBgMdHR00N3dzZEjR+S6c3Nzct1snX6n2KdlexaLhZ6eHjo7O+WyQCDA\nwMAAAwMDzM3NZdWfmbDT7ZPQhdPv3buXs2fP8v7771NZWQlAJBKhsrKS2dnZDU7R09PDt771LcUH\n89lnn+HxeBROUVNTQ2dnJ++88w4nTpyQy2/fvk0wGGRwcFD+EMvKyujo6ODtt9/m7Nmzct2hoSFW\nVlYYHh5W3GV3o31attfW1sb58+e5fPmyXDYzM0NJSQkTExMKp8ikPzNhp9snoQunj0QiuN1uxsbG\nqKioACAajeJ2u4lEIoq60WiUhYUFJicnCYVCcrnL5dow7FldXcXv9+N0OhVD5ZmZGRYXF+WhGsDa\n2hrBYJCZmRnsdrtcPjU1hd/vJx6P73r7tGwvFAoxNzenqDs/P4/X62VlZUVRN5P+zISdbp+ELpze\n5XLx5Zdf4vF45OHM6uoqz549Y35+XlHX7/djtVoJBoOKZ6nx8XGcTqeibjgclr95Gxsb5XLpg0r9\nYKT2AMXd1Ov1Mjo6Sjgc3vX2adme0+mkr69PYWMoFGJ0dBS/36+om0l/ZsJOt09CN07v8Xi4d++e\nonx1dVUeTkn4fD4ePHjAo0ePKCkpkcvj8fiGuktLS9hsNp4+fSpPiEh1pVdqW6Ojo0xMTCjqJhKJ\nTY+9G+3Tsj2Hw8HMzAy//e1v5bK1tTVWV1c33DUz6c9M2On2SejC6aUOfXGYsxmJRIJEIpHWpNra\n2hqxWCztCTi1OvVFdop9Wrb3olOlUzed/syEnW6fhObr9AKBQF/o4k6fCSUlJRgMBgwGg2L4mwnS\nECyXyTmtUMO+ncLa2pr8OaVOoukF6XNKHd5nQqHsKzqnb2hooKOjg/b2dsVEVyZIEzOjo6O6c3w1\n7NsphEIh7HY7drudhYWFQp+OAmnJrr29nX379mV1jELZV5RO39PTw6VLl2hqasrqGAMDA0ByRlyP\nTp+rfTsFt9vN9evX8Xg8unN6aXPOxYsXOX36dFbHKJR9unD6yspKqqurqa6uloe0a2trRCIRwuGw\nYkKjtraWo0eP8sYbbyjiozPFZrNlPSzTErXs2wlIodi1tbWFPpUNGAwG9u/fT09PDxcuXMjqGIWy\nTxdObzab6ezspLOzU7HtcGhoiKGhoQ3r0wKBIHt04/Rnz57lm9/8prwDaXl5mV/+8pd4PB7h9AKB\niujC6Wtrazl48CCnTp2So5vC4TAPHz7M29BHL1F2gu3RQ5RdMaMLp9cDeomyE2yPHqLsihnh9Ovo\nJcpOsD16iLIrZoTTr6OXKDvB9ughyq6YScfp/z3wDjAPnFovawT+FjhEUuHmvwL8m725WNBLlJ1g\ne/QQZVfMpOP0fw3878D/lVL2XeAa8COSwpXfXX8VLXqJshNsjx6i7IqZdJz+Fsnc9qn8HiDtSPgP\nwOfk4PRutxur1UpdXZ08rFpZWWFgYACPx5PtYTNGD1F2gu3RS5RdsZLtM70ZcK3/7lr/O2uk4dr0\n9DQGgwFIOqDT6cTlcm3zboFAkAlqTORJ+llZ43a7cbvd3L9/X4XTEQgELyNbp3cBe4E5YB/JSb5N\nkSStAEUmUIFAoB63b99WaNm9jGyd/u+BPwR+uP7zZ1tVTHV6oWUnEGiD2lp2PyE5addMUo/+fwH+\nHPi/gf+G50t2eSEWixEMBnN61vd6vYTD4ZzWXSORCD6fj5mZGbnM5XIRDAZzmghSw76dgpb96fP5\nclrTX1tbY2lpCa/Xq7gGMkEN+7IhHaffoES7ziU1TyRdpMSRgEIMIhPGxsZyTqDhdDq5efOmYnUh\nEAhgtVo3ZDfNBDXs2ylo2Z+PHz9mampqq7dti7TEe+3aNUZHR7M6hhr2ZUPR7ciTUkRPTEwodk1l\nQigUwufz5eT0DoeDYDDIw4cP5bJYLIbP58vpQ1TDvp2Clv0ZCATw+XxZHzcejzM6OorH48k6KEwN\n+7Kh6Jw+EokQiUSYnZ0t6HkEAgECgYDqx9WLfTsFrfozkUjg9Xrxer2qHjcf6C91jEAg0BTh9ALB\nLkMXw3uTyURraytms1ne8xyPx5mfn8flcimG0bW1tbS2ttLa2iqHVUJyg8/8/LxiYq2yslI+bmpi\nDL/fj8vlYn5+nkQiASSTaJjNZlpbWzGZTHLdUCgkn0fqts/m5mbMZjPNzc1yWTQalY+bmsihGO3b\nDNGf+rMvG3Th9Gazmd7eXnp7e+WoqZWVFfr7+7lz547CSJPJRE9PD729vdTX18vlVquV/v5+xYco\nJUV47bXXOHDggFw+PDxMf38/Xq9X/hClePre3l5eeeUVue7U1BR37twhGAwqPsS2tjZ6e3vp7u6W\ny3w+n3zOqR9iMdq3GaI/9WdfNujG6c+ePcuHH36oSJcFMDExwcjIiFw3Vco5Nd94XV0dMzMzWK1W\nuaympoauri7effddurq65PK+vj68Xi/379+X10jLysp45ZVXePvtt3nzzTfluoODg4RCIR49eqSY\nZbVYLFy4cIH33ntPLpuammJlZYWRkRGmp6eL2r7NEP2pP/uyQRdOv7CwgM1m4/r164oou6GhoQ3L\nKsFgkJGREW7evElDQ4NcPjAwsGEDxvLyMuPj49y+fVuxJiut0aYu2cXjcZxO54Zv3YmJCcbHx4lG\no4pjz8/PY7Va5SQOkNz08+zZM4XEdLHatxmiP/VnXzbowukzibLz+XxYrVY8Hg/V1dVy+fT09Ias\nuVJ89OLiomJo53a7cTgc8lBNam90dJRoNCqLYUByac7hcGxIouFwOOjr61NszAiHwzidzi2lh4vJ\nvs0Q/ak/+/TIWioOh2PtBz/4wdqpU6ekyDzxEi/xyuF1+vTptR/+8Idr09PTCl9b//+miCU7gWCX\nIZxeINhl5PWZ3mAwUFdXR0tLy7Y6bVtp2WXanqSRl7rvOhqNytszs420q6qqwmg0KkQRMkFL+/RM\nLBaT+z7b2IfNtA8zZXl5mXA4zPLyclbvLykpobq6GqPRqJicy7d9LS0t1NbWys/+6ZBXp5fWQVdW\nVnj11VdfWlcNLTuj0Shr5LW0tMjlDocDm83GkydPss6J19bWRmdnJ8eOHcvq/Vrap2fm5+dlu1+c\ntU6XzbQPM2V4eJihoSE5A3KmSEt2nZ2dWCwWuTzf9u3bt4+uri7FJOW2557VWWWJJCawb9++bb9h\n1dCyk9Zdv/nNb9LR0SGX3717l2g0ysjISE5Of/78eb7xjW9k9X4t7dMz0hrz+Ph4Tk7xovZhpty4\ncYNgMJi100uKSJcuXeLMmTNyeb7tq66uxmQy6dfpy8vL5S2R26GGlp3U3rFjxzh16pRcvrCwQGNj\nY9ZDQ4D6+noOHz6ctTa5lvbpnXv37uX0OLKZ9mGmjI6OKpblMqW0tJSmpiba29s3XAN6sO9liIk8\ngWCXIZxeINhlCKcXCHYZ6Ti9Bfg1YAMeA/9qvbyRpLTVCPAPgGnTdwsEAl2RjtPHgP8e6ALOAv8C\nOMlzPbtjwA2KXMtOINgtpDN7P7f+AggBT4ADqKxnly719fVYLBYOHjyoWKaYmZnB6XRmnY7YYDBg\nsViwWCyKRAeBQACn04nT6cx6I0cmaGWf3tm/fz8Wi0WxaSsSich9v7i4qPk5VFdXy9fAnj175HK3\n243T6dwQaZcJerBPItMlu8NAD9CPynp26SLFR58/f14RKvnll19y69atrJ1C2mxx7tw5hRLP5OSk\nHE+dD6fXyj69I+17eO211+Qyr9fLrVu3CAaDeXEKad/DuXPnaGtrk8ttNht9fX3MzMxk7fR6sE8i\nE6evBX4K/DEQfOF/L43qUZO6ujpOnDjBxYsXFUkRYrFYTskFpDv92bNnOXfunFw+ODjI9PQ0d+7c\nyem800Ur+/TO3r17OXPmDO+8845cNjU1Jceh54Oqqira29s5f/68IolGbW0t4+PjGAyGrIUp9GCf\nRLpOX07S4f+G5xJWaenZpcpavfXWW7z11lvZnek6Uh7zyspKxeaKBw8eZL2zDZTiBZOTk3L51NQU\nQ0NDOamhZIJW9ukdh8PBzZs3FXc8n8/HwMBA3vLCh8NhbDYbv/jFLxSOODw8nLM4itb2ff7553z+\n+edp1U3H6UuAvwKGgL9IKU9Lzy7V6dVASoowMzOzIdHh3NzcS975ciTxgmAwyL179+TyUCiEy+XK\nm9NrZZ/ecTqdrKys8OTJE7lseXkZl8uVN6eXkmi8KGDh8/lwuVw5KyJpad+LN9Rctex+B/gYeARI\nX39/RoH07EKhEKFQCLvdrupx4/E4MzMzBX9m1so+vSPJlReSaDSKw+HA4XCofmw92CeRjtP/hq2X\n9gqiZycQCLJH7MgTCHYZmkfZDQ4OZvW+5eVlpqenFZlG9YSUALGQ9sViMdxud9bhoS9SWVmJ0WjE\naDTKEYhra2uEw2HC4XBaGXO3Y3R0FLfbnZM8cygUYnp6GpvNlnVorcPhyOsyWSaoYd/L0NzpP/nk\nk6zeF4vFsNlsus3+6XQ66evry3pyTQ37pNlmQJUkGi0tLRw6dIi2tjZFFlaHw8Hk5KRCGCJbpCQT\nuUyMStlig8FgTkk09LoaooZ9L0Nzp//xj3+c1ftS7zB6xOFw4PF4sl6/V8M+abZ5fHxclXRZHR0d\nnDlzhpWVFflii8Vi3L9/n/v376syuRiLxQiHwzk7fTAY5NGjR1nnRJBSlekRNex7GZo7fapowE5C\nyoNWSOLxOMFgkGDwxb1S2VFXV4fP52NpaYnl5WUWFxfx+/3MzMywurpKZWUli4uLLC4uqjLUz5Zo\nNEo0GlVF+EGPaG2fLsQuBPpjYWEBu92O0+mktrZW3jc+NjbG2NhYQZ1ekBti9l6wKV6vl+HhYe7e\nvUsoFMJisXDmzBkOHTqE0Wgs9OkJckAXd/rm5mY5ukmaQJK0wpxOp2ICSS9RdplETWllXyZRYenY\nl0oikWB1dZWVlRXW1tYoKyujvr6ekydPUldXp7Av2/aKuT8zRSv7skEXTr93715ef/11zp07pxDs\n6+vrIxqNKozUS5RdJlFTWtmXSVRYOvZth5TC/PTp07Juei7tFXN/ZopW9mWDLpy+qamJU6dOcfny\nZYU0r8/nk5ekJPQSZZdJ1JRW9mUSFbaVfY8fP2Zubg6r1YrBYJCdubS0FIPBQHl5OQaDQRZ3OHTo\nEIcOHVIEA2XS3k7pz0zRyr5s0IXTz83N8eWXXwLIS0UrKyt8+eWXG9ax9RJll0nUlFb2ZRIVtpV9\nPp+PUChEZ2cn9fX1tLa2UlpaSmNjI8ePH6e2tpajR4+yZ88eeYlwcHBQoQyUSXs7pT8zRSv7skEX\nTu9yuejv72diYkK+08TjcVwuF/PzyohdvUTZZRI1pZV9mUSFbWWf0WikqamJzs5OTCYTe/bskXO6\nl5WVcfDgQerr69mzZw/hcFievU+1MZP2dkp/ZopW9mWDLpze7/fj9/t5+vTptnX1EmWXSdSUVvZl\nEhW2lX0Wi4XXX3+dtrY2TKbnuU3r6uqoq6tT1F1YWMDhcHD37t1t91/s9P7MFK3sywaxZCcQ7DKE\n0wsEuwxdDO8FhSMWixEIBJiZmdlWcDEQCBAIBHKKkBMUHuH0u5xwOMzk5CSrq6vbiiVK4cB6DVQR\npIdw+l3O0tISk5OTzM3NKTbcbEYikSAajbKyspKnsxNogeZOf+HChe0rbUI8Hmd+fh6Xy0UgEFD1\nnEwmE8eOHePcuXMFu4DVsK+yspLW1lbMZjM1NTUqn6E2hEIh2e5s+95kMsl2b/dFpRWVlZW88sor\nihUPtVDDvps3b275v+2cvgq4CVQCFcD/QzIpZiPwt8AhnifF3DSl55UrVzI9XyC5GaG/v587d+6o\n7vRms5mzZ8/S3NzM6uqqqsdOFzXsk7bFvvbaaxw4cEDlM9SGqakp7ty5QzAYzNrpzWYzvb299Pb2\napJkIh2kbcatra2qH1sN+3Jx+mXg60B4ve5vgDdJSlpdA34E/ClJOatNJa0+/vjjjE8YkJ8bJyYm\nVBd5MJvNNDc38+qrr6p63ExQwz5pr/i7776r2DaqZwYHBwmFQjx69Cjr1M/Sl/aHH3647TyElpSV\nlWnypaOGfX/0R3+05f/SOWNp1qYCMAA+MtCxyyUMs6KiQo4yUhODwYDBYNAk/1gm5GpfSUkJ5eXl\nVFVVFU24a1VVFeXl5TllhJE+O6PRWFCn1wqt7UvngaEUGCCpaCNJVhdEx04gEOROOnf6BNAN1AO/\nIjncTyVvOnaQvLtJd+rUu0U8HieRSOS0P1oPqGGfVK/QfSHZkc5klFQvdeSztrYm25Ea4FOM6Mm+\nTB5IAsAvgTOkqWMH6mvZNTQ00NHRQXt7u2LGenx8HLvdzsTERE7HLzRq2OdyuRgdHcVutxdsorKs\nrEy2IzWkdSssFgsdHR0cPnxYLpP2zNvtdhYWFjQ8W+3R2j41teyagVWSM/PVwNvA90lTxw7U17KT\nkiJcunSJpqYmufzWrVtEo9Ed4fS52ielUL5x40bBliQrKyt5++23qaurS8vppSQTb775plzmdru5\nfv06Ho+n6J1ea/vU1LLbR3KirnT99TfADZKadnnXsYPkxdTS0kJ7eztm8/OpBLvdrgiHLFbUsM/n\n8zE8PMzNmzcV2WnyidFopK2tjTNnzqRVv7a2lgMHDnD8+HG5rK6ujpaWFkVIbLGiJ/u2c/pB4Gub\nlC9QIB07n8/HgwcPABShn48fP94R6bZ3un1b4XQ6uXnzpiIVVCAQwGq15k21Vkv0ZF/RbcOVMqFM\nTEwoBB4CgcCOyIO+0+3bCofDQTAY5OHDh3JZLBbD5/PtCKfXk31F5/SSyMTs7GyhT0UTdrp9WyFF\n8O1U9GSfiKcXCHYZwukFgl2G5sP7l238lzCZTJjNZjkTq1pIOc/u3bunuyWfaDTKs2fPdsTzar7x\n+/2MjIxw69atgm+lfhFJCkztpdJEIiEnxsz1mtGFVPXx48fp7e2lqalJVacPh8MMDQ2xsrJCY2Oj\nasdVg9XVVZ49e6ZKdtPdhrQPwePxFCzKbiu8Xi+jo6OqJxqRUor39/fz7NmznI6lC6nq8+fP09TU\nxJkzZ1SRXJaQUho/ffq0YHHXL2N1dbVgO+aKGZfLhcfjUaTW1gvSFmi1P1fpJnHt2jV+85vf5HQs\nzZ0+nW+8aDSqycW/trZGLBYTOd12GNJ+9d2WwWd1dZVoNJrzKEJ/tz+BQKAp+nogSoNMotC2qrtV\nFNpmUWFbRUJlEjUl1Xsxdl6qm0gksrJvJ6GH/tTL9aI1Ref0mUShGY1GuW7qRN7MzAx2u12hT7ZV\nVJjX65Xrpg6rMomaMpvN8rGliSdpYsZutys24uz0KMKt0EN/6uV60ZqidPp0o9Bqamro7Ozk0qVL\nHDlyRC4fGBgAkh98qp56R0cHFy9e5PTp03Jdu93OjRs3NqR+ziRqSkp/dPHiRXmJKRqNcu3aNYLB\n4IaLdCdHEW6FHvpTL9eL1hSd02cShVZWVkZjYyOHDx9WRDd5vV7q6+sVQ7jS0lLq6+tpa2tT1I3H\n4zQ2Nm5YGsokaspoNLJ3716OHTsm/y8SiTA4OEh1dXXW9qlNWVkZFRUVVFRUyH2ztrbGysoKKysr\nmq406KE/9XK9aE3ROX0mUWiS9HBFRYVieWdsbOyl0sqjo6Ny+ezsLDabbVOV1XSjplwuF1988QXL\ny8vyxRCLxbBarZuqrBYqyq6+vp6DBw9y4MABhVrq9PQ009PTeL1ezdrWQ3/q5XrRmqJz+kyi0JaW\nlhgaGmJ2dlaRYDAUCuHz+TaVVn5RpjgSieD3+zfVU083akqSHh4eHpadKZFIbFq3kFF29fX1dHR0\ncPr0abntlZUVBgYGCAaDmjq9HvpTL9eL1hSd02cShRaLxZifn09r11sikcDr9aZ9YWcSNSXJJTud\nzm3rFjLKrqqqisbGRtra2lhdXWVpaYl4PE5tbS2tra1Eo1GWlpYIh8Oq733QQ3/q5XrRmqJzekF+\n8Hq9TExMMD09jcFgoK2tDbPZzOTkJBMTE7q5gAWZI5xesCkej4enT58yNDTEV7/6Vb761a9SX18P\nJGedhdMXL2JHnmBTYrEYwWBQfpatrq7GZDJhNBo1ESAR5I90nd5AMhnmz9f/biQpazUC/AOgvoqf\nQCDQhHSd/o+BIZ6LWnyXpNMfI5kdd1NJK4FAoD/SeaY/CHwT+F+Bf71elraW3fvvv79tA1/5ylc4\nfPiw6sPG6upqLBYLFouFPXv2qHrsXInH4zidTpxOp2LtNt8YjUYaGxtpamri6NGjNDc3U1JSgslk\noqOjg/Lycg4dOqRY4y40zc3N8ueqt0eNQCAgf65qph83GAwcOXKEc+fOpZUb4tNPP93yf+k4/b8F\n/gRI9Zq0tew++uijbRtoaWmhra1N9Zh3SdX13LlztLW1qXrsXFlZWaGvr49oNFpwpz98+DAnTpxg\n3759ciKT5uZmTpw4wf79+2lqaqK+vl43oax79+7l9ddf59y5c6rmX1CDyclJ+vr68Hq9qjq9tNe/\nsrKS7u7ubevn4vTvkpSssgJvbVHnpVp2H3zwwTZNaEd1dTXHjx/n0qVLnDp1qmDnsRnhcJhgMMiT\nJ08Keh5GoxGLxUJ3dzcNDQ1yeUNDg+JvQNPNOZnQ0tLC1772NX7/939fd6q1Dx8+xOVycffuXVWP\nK93pU2MCsmU7p3+D5FD+m0AVybv931BALTuBQLARNbXs/s36C5LP8P8D8M+AH1EgLTuBQLARNbXs\nXkQaxv85BdKyKy8vp7q6murqasUkzvLyMuFwuGDabWqRb/vi8TiRSCStzTaBQIBIJKJJIo+qqiqM\nRqNiuC6dWyQSKfqUZ3qyLxOnv7n+ggJq2TU0NHDy5Em6uroUgQ7Dw8MMDQ3lnCm00OTbvqWlJTmm\n/MWw1BcJh8NMTEyonukVkvHmnZ2dHDt2TC5bXFxkaGiIJ0+e4Ha7VW8zn+jJvqLbhmsymejp6eGd\nd96hpaVFLr9x4wbBYLDonT7f9kmO7Ha7t13+isfjLC0tsbS0pOo5wPMkE9/4xjfkMpfLRWVlJbOz\nszvC6fViX9E5fUlJCSUlJZSWliqW+KTyYkcN+6SEDV1dXaovs6WrH1BVVcWBAwcUKaq2Y6d+phJ6\nsa/onN7n82G1WllZWVFsGBkeHk4r1FLvqGGflE6qrq6uYHn1y8vL6erqUmSreRlOp5O+vj7m5ubk\nMmn4uxNUgPRkX9E6/fDwsGI4GolENHnWzDdq2Gc2m6mrq+OrX/1qXrOsplJSUoLRaMRoNKZV3+Fw\n4PF4uHPnjlwWj8cJh8M74nPVk31F5/SxWExXCQnURg37Kisrqays3LC5Rs9Is9g7FT3ZJ0JrBYJd\nhnB6gWCXofnw/mUb/1+GlJBRi2AUj8cjR0IVSjFGDfsikQgjIyP84z/+oyIjq56ZmJhgZGQkp01G\nbrcbq9VKXV1dwQJuDAaDHOnX3Nys6rG1tk9zp7969WpW71tdXcXpdOJyubavnCFzc3N88cUX3Lp1\nq2A7vdSwT1LlXVxclFNZ6Z1AIIDD4chp8kqSqpby9xWCiooKzp8/T2VlpepOr7V9mjv9T3/6U62b\nyBi3282DBw/42c9+VtTbdiORCMPDwwwPD2d9jK302zajUNprL+J2u3G73dy/f79g52A0Gqmrq+Pk\nyZOqH1tr+4pu9l6gLg0NDbS3t9PR0bHt8trS0pKs06Z1Dn6Bdgin3+VspfW2GfPz89y4cQOPxyOc\nvogRTr/LkbTejh49Sltbmxzhl6plJ60xT0xM8OjRo7xrrwnURTj9LkfaAQjQ1dVFZ2cnJ0+e3CAB\nPTQ0xOPHjwuivSZQF+H0u5xUrbe5uTkqKys3qLCOjo5y/fp1vvzyS/x+v3D6IkcXTl9TU4PJZMJk\nMikECaULLDWUs6qqSq77oiCh3+9ncXExq3MoLS2Vj5saGba8vCyfR+ryXn19PSaTSZFlNxaLyXVT\nVwW0sq+8vFyu+6LgonRsaZZ9O/uePXtGTU2NvPbc0tKCyWSirKwMr9crT+CZTCYsFkvO7RV7f2aK\nVvZlgy4LExUAAAAXb0lEQVSc3mw209PTQ3d39wbp4YGBAcbHx+W6DQ0NdHd309PTo4hCs9lsWK1W\nbDZbVudgMBjo6Oigu7tbkXxwbm5OPo/Ui9RisdDT00NnZ6dcFggEGBgYYGBgQBFNpZV9RqORrq4u\nuru7FdFsY2Nj8rGlKLt07JPUYIPBIKdPn95QV+32irk/M0Ur+7JBF06/d+9ezp49y/vvvy9PEkUi\nETnBwIsfYk9PD9/61rcUH8xnn32Gx+PJ2umlFMNvv/02Z8+elcuHhoZYWVlheHhYcVeQkiJcvnxZ\nLpuZmaGkpEQeKmttX01NDZ2dnbzzzjucOHFCLr99+zbBYJDBwUH5Ik3HPpfLRTAY5NGjRywuLtLc\n3KxwWLXbK+b+zBSt7MsGXTh9JBLB7XYzNjZGRUUFANFoFLfbvSEyKRqNsrCwwOTkJKFQSC53uVw5\nDXvW1tYIBoPMzMxgt9vl8qmpKfx+/4btuqFQiLm5OUXd+fl5vF7vhsQVWtm3urqK3+/H6XQqhq4z\nMzMsLi4qhqJb2efz+WhoaODVV18lGAzK5WazeUMCDDXa2yn9mSla2ZcN6Tr9BLAIxIEY8BpJPbu/\nBQ7xPDlmVjM80rZDj8ejmDV+9uzZBq1waeIpGAwqLsrx8fGckmhI7QGKb3+v18vo6OiGbaNSUoTU\nNkOhEKOjoxsmurSyLxwOy3fO1Iw2kqOlOtZW9lVXV9PY2Mjly5cVF/qRI0c4ePCg6u3tlP7MFK3s\ny4Z0c/WMA2dIJsSU+BHgWf/5p0ADG6Wt0vpqlLaBSgZKrK6usrq6SiKRkMtKS0vluqnbRuPxOKur\nq4oPxmKx8NFHH3HlyhWF2MWvf/1rrl69ytWrVxUTKGVlZZSVlSlSGiUSCfnYqd/00jmk7o1eW1uT\nzyH1nLWyr6SkRD6H1HOWtsq+OBTdzL6uri4++OADPvjgA/bu3bvBvmg0ytWrV/nkk0+4detWzu3t\nlP40Go18/PHHXLlyhQsXLsjlDx8+5JNPPuHHP/4xMzMzmtu3DZv6dybD+xcPkLae3XZInZpOPrdE\nIkEikdAkUEbq1HSQzjmTumrbt7a2RiwWS7svNrPP7XZjs9loamri2LFjWCwWDh48iMvlwul0MjEx\nwcDAAF6vV5X2tqIY+zMTtLIvG9J1+jXgOsnh/f8J/Dsy0LMT6Bdpc47X6+XMmTOcP3+e/fv3y8PR\nL774AofDoUm0o6AwpOv0vwPMAi0kJaqfvvD/LfXsUocz0lArXaRvvAyGM3lDGgK+OFzLNQptq6g3\nqR9yea6Ujps6dI1Go4yOjjI6Osrq6ipHjhwhHo8zNzfHgwcP+NWvfiXX3Uo3TjqvXM+t2PpTK6R+\nyNRXUv3kZXal6/Sz6z/dwKckJ/LS0rP7zne+k2YTSqTtn3a7ndnZ2e3fkGcsFgsdHR0cPnxYLguF\nQtjtdux2OwsLC1u/+SU0NDTQ0dFBe3v7hoknu90uC1NkirSE1t7ezr59+zatc/z4cTo6OigrK5OX\nmNKR+JYmukZHR7N2omLrTy0xm83yOb/4XJ8uf/mXf7nl/9I5ohEwAEGgBvgnwPeBvycNPbsrV65k\ndrbrRKNRrl27RjAY1KXTS07x5ptvymVut5vr16/j8Xhyukg3i3q7desW0Wg064tU2ixz8eJFTp8+\nvWXbZrMZg8FAW1sblZWVis0kWzEwMAAkHSlbpy+2/tQSKYX5xYsX5SW7TMnV6c0k7+5S/avAPwD3\nSEPP7utf/3r6Z5pCOBzG4XAUNFHCy2hpaeErX/mKwj6n08nY2JhCjipTamtrOXr0KG+88Qb79++X\nyxcWFmTnygaDwcD+/fvp6elRzDZvRUtLi0JhZztsNpvisSFTiq0/taShoYHjx49z4cIFTaS403H6\ncaB7k/KC6dkJBILsEdlwBYJdhi624WaCXqLsMkGrqCkto8JyRfSnkkyi7LSm6JxeL1F2maBV1JSW\nUWG5IvpTSSZRdlpTlE6vhyi7TNAqakrLqLBcEf2pJJMoO60pOqfXS5RdJmgVNaVlVFiuiP5UkkmU\nndYUndPrJcouE7SKmtIyKixXRH8qySTKTmuKzul9Ph8PHjzg0aNHm0ZNZYu0A3BiYmLLqLBscblc\neDwe7t27t6HNXI4rKdw8ffp006iwQju96M/nOBwOZmZm+O1vfyuXpUbZ5ZOic3q9RNllglZRU1pG\nhamB6M/nFPpLOBWxTi8Q7DKE0wsEuwzh9ALBLkM4vUCwy8jrRF48Hpd10babLIlEIgQCAaLRaE7t\nBYNB3G63Il+Z1+slHA4XdB0736ytrbG0tITX61X0hRqI/nzen263m1AolNOkXTQaJRAIMDs7u61u\nYHl5uaw/mG7Sjbw6vbQOOjQ0hNvtfmldaVtlruuuNpuNiooKxfLO2NhYTgkfihFpCe3atWuMjo6q\nemzRn8/7c3Z2FpvNltMmIZfLxRdffMHy8vK2STRaW1vp7Oyks7Mz7RDkvDq9tA762WefbXvhJRIJ\nfD5fThsXlpaWGBoaYnZ2dkMAhc/n21UXqaRJ5/F4copP3wzRn8/7MxKJ4Pf7c3b6/v5+hoeHt81R\ncOzYMSCZslyXTh+LxZifn2dkZITBwcG8tadGrvBiJ5FI4PV68Xq9hT6VHYGW/RkKhQiFQmnvMH31\n1Vcz2lsgJvIEgl2GcHqBYJehi224JpOJ1tZWzGaz/AwTj8eZn5/H5XIRCATkurW1tbS2ttLa2qqY\n2XS73czPz+PxeOSyyspK+bipwTl+vx+Xy8X8/LycNri0tBSz2Uxraysmk0muGwqF5PPIdttnJvZl\nQr7tE/2ZvX3Nzc2YzWaam5vlsmg0Kh83NUJUK/sk0nV6E/CXQBfJ/PbfAZ6hkpad2Wymt7eX3t5e\nebZyZWWF/v5+7ty5ozDSZDLR09NDb28v9fX1crnVaqW/v1/h9FJShNdee40DBw7I5cPDw/T39+P1\neuUPUYr/7u3t5ZVXXpHrTk1NcefOHYLBYNYXaSb2ZUK+7RP9mb19bW1t9Pb20t39PN2kz+eTzznV\n6bWyTyJdp//fgM+Ab6+/pwb4n0gKX0hadt8lS1krKeXvhx9+KM+yS6GXExMTjIyMyHVTk2ik5m+v\nq6tjZmYGq9Uql9XU1NDV1cW7775LV1eXXN7X14fX6+X+/fvyBEhZWRmvvPIKb7/9tiIN8+DgIKFQ\niEePHmW9kpCJfZmQb/tEf2Zvn8Vi4cKFC7z33nty2dTUFCsrK4yMjDA9Pa25fRLpOH09cI5kbnuA\nVSCAilp2CwsL2Gw2rl+/LicvWFlZYWhoCJ/Pp6gbDAYZGRnh5s2bNDQ0yOUDAwMbpJeWl5cZHx/n\n9u3bTE1NyeWPHz9mampKscQUj8dxOp0bvnUnJiYYHx/PaZNQJvZlQr7tE/2ZvX3z8/NYrVZFHnuv\n18uzZ88UyWC0tE8iHac/QlLZ5q+B08B94L9DRS07KSnC9PS0vKtodXUVp9O5wZEl7TWPx0N1dbVc\nPj09vWGJQ9oXsLi4qHgUcLvdOBwOhQyQtNkiGo0q8qEHAgEcDocqSR/SsS8T8m2f6M/s7XM4HPT1\n9Sn2p4TDYZxO55ZS1WrbJ5GO05cBXwP+JXAX+As2l6TedA/m9773Pfn3rdRS3G43brc7LWGLQCDA\n4OBgWuv8kUiE4eFhhoeHt60bj8cZHx/POr/ay8jEvkzIt32iP5+TqX3T09OKIfzLyMa+27dv8/jx\n47TqpuP0U+uvu+t//x3wZyQ17LbVskt1ekmxRCAQqMsbb7zBt7/9bfnv73//+1vWTWedfg5wAsfW\n/74E2ICf8/w5f0stO4FAoC/Snb3/b0lq2FUAdpJLdgbS0LJTm2yiil4kGo3K0X56iwyLxWIEg8FN\n5zJy2c9dUlJCdXU1RqMxa1HErVCjPyORCD6fTxGx5nK5CAaDukwHpkZ/ZhJ1qibpOv1D4L/YpDzv\nWnYNDQ2cPHmSrq6urANHHA4HNpuNJ0+eFCwv/FZIiT8BhZiHNIOcLdISU2dnJxaLJefzTEWN/nQ6\nndy8eVOxzyIQCGC1WvOeLTYd1OjPxcVFhoaGePLkybZRp2qiix15mSBtznnnnXcyUlVN5e7du0Sj\nUUZGRnTn9FKK74mJiQ2yXbks10iKM5cuXeLMmTNqnKqMGv3pcDgIBoM8fPhQLovFYjlHWmqFGv3p\ncrlkgQ7h9C+hurqaffv20dnZqZAezoSFhQUaGxsVKbT1gjTcm52dVfW4paWlNDU10d7evqU+fbao\n0Z+BQCDnnWb5RI3+dDqdPHjwQBM56pchAm4Egl2GcHqBYJchnF4g2GUIpxcIdhnC6QWCXYZweoFg\nlyGcXiDYZQinFwh2GcLpBYJdhnB6gWCXIZxeINhlCKcXCHYZwukFgl2GLqLsampqMJlMmEwmObl/\nIpHA7/fj9/sV2Ub1THl5OS0tLYo86GoRCATw+/0sLi6qfmw9UF9fj8lkYs+ePaoet6Ojg5aWFkWY\n8m5HF05vNpvp6emhu7tbTu4vSVUPDAxoklxRCySxBED1+GibzYbVasVms6l6XL1gsVjo6enZMnlq\ntkhSzqmZk3c7unD6vXv3cvbsWd5//31ZqioSicgJBorF6SWxhCNHjqie/uizzz7D4/HsWKdva2vj\n/PnzXL58WdXjlpeXYzQahdOnkI7THwf+Y8rfR4H/GfgxKslaVVZW0tDQwIEDBxSKHiaTSaFXp3cM\nBgN1dXWKNFdq0dTUtKMv3Orqapqamjh48GChT2XHk85E3jDQs/46A4SBT0nmvr9GMkvuDbJUtxEI\nBPkl09n7S8AoyZTYv0dSzor1n7+v4nkJBAKNyPSZ/p8CP1n/XTVZKz1gMBiwWCxYLBaFnHAgEMDp\ndOJ0OlleXi7gGW6kurpaPufUWW+3243T6dygv5ZPRH/ql0ycvgJ4j6RC7YtsKWtVLEgpjc+dO6eY\nQZ6cnJRVS/V2kUoTh+fOnaOtrU0ut9ls9PX1MTMzU7CLVPSnfsnE6X+XpHiltBblIkNZK7WXY9RE\nujOdPXuWc+fOyeWDg4NMT09z586dAp7d5lRVVdHe3s758+cV0sq1tbWMj49jMBgKJhQh+jO/qK1l\nJ/EHPB/aA/w9STmrH/ISWati0bKTVEivXbvG5OSkXD41NcXQ0FBO6jJaEQ6Hsdls/OIXv8Bqtcrl\nw8PDjI6OFvSuJPozv2SiZZeu09eQnMT75yllf04BZK20Ih6PMzo6SjAY5N69e3J5KBTC5XLp8iKV\npJU9Ho9C7cfn8+FyuQp6kYr+1C/pOv0S0PxC2QIqyVpFo1ECgQCzs7OKzTmBQIBoNKpGE9sSj8eZ\nmZlRaKnpnWg0isPhwOFwFPpUNiD6U7/oYkeey+Xiiy++YHl5ecM23Pn5TacKBAJBlujG6fv7+xke\nHlYE3OhVx0wgKGZ04fShUIhQKITT6dy27vLyMnNzczx9+hSv15tVe+Pj4ywsLOQkUx0IBHA4HAwO\nDmZ9jExwOBw5RdglEgkWFhYYGxujsbFRxTMT/Zltf87NzeFyufK+dKkLp88En8+H1WplZWUl6z3u\nk5OT2O32nCZmnE4nfX19zM3NZX2MTBgeHk7rS3ErpIm169evMzIyouKZif7Mtj8lqep8j2aL1umH\nh4cxGAxZHSMajRIOh3O6SB0OBx6PJ2/rzZFIhHA4nPX7V1dXefbsGdPT06oHMYn+zK4/4/E44XA4\np/PIhqJz+lgspgtZY0lSulhYW1tjaWlJtwlJRH/mD5EuSyDYZQinFwh2GcLpBYJdhnB6gWCXoYuJ\nvNLSUgwGw4bZ+Hg8TjweJ5FIyGUlJSVy3ZKSEkXdRCKhmEHeqq5U78XZZqmutEEIkhM2Ut3UdWip\nXuo5b1W3GO3bDNGf+rMvG3Th9GazmY6ODtrb2+VtuFKUlt1uZ3Z2Vq7b0NAg162pqZHLx8fHsdvt\nTExMyGVGo1Gum7qBYmZmBrvdroicKisrk+vu27dPruv1euW6qUsrFouFjo4ODh8+LJeFQiHsdjt2\nu52FhYWitm8zRH/qz75s0I3Tnz17losXL1JRUQEk136vXbtGMBjc8CH29PRw6dIlmpqa5PJbt24R\njUYVH2JNTQ2dnZ1cunSJI0eOyOUDAwNA8oOXPkSDwUBHRwcXL17k9OnTcl273c6NGzeYnp5WfIhS\n9tY333xTLnO73Vy/fh2Px7PhQyw2+zZD9Kf+7MsGXTi90Whk7969HDt2TBFlNzg4uCEDbGVlJS0t\nLbS3t2M2P8/QZbfbFeGQkPw2bmxs5PDhwxw/flwu93q91NfXK4ZwpaWl1NfX09bWpqgbj8dpbGyU\nv3ElamtrOXDggKJuXV0dLS0tGzZrFKN9myH6U3/2ZYMunD6TKDufz8eDBw8AFNtwHz9+zNTUlKKu\nlBShoqJCEdM9Nja2ISlCatKH0dFRuXx2dhabzbZh44jT6eTmzZt4PB65LBAIYLVaN2yrLEb7NkP0\np/7sy4aS7avkxFrqBIXT6eTq1at88sknisCK2tpaGhoaNshaSVF2oVBIrltdXY3JZKKhoUEhVRQI\nBPD5fIqdeuXl5TQ0NNDQ0CDn04fks5TP58Pn88kTKKWlpfI5pN4BIpEIfr8fn8+nSJVUX19PQ0MD\n9fX1clksFpOPmxpEUYz2bYboT/3Zd/r0aa5cucLHH3/M/v375fL1UYnW/r0pa6k4HI61H/zgB2un\nTp2SEmmKl3iJVw6v06dPr/3whz9cm56eVvja+v83RazTCwS7DOH0AsEuQ/OJvM8//5y33noLSM6k\ntrW18eqrr6qeyEHC7/djMpk0ObZoT7Snt7ba29uxWCzy0l465NXppXXQiooKxbqkmvz85z/nvffe\n0+TYoj3Rnt7aampqoqOjA6PRmPZ78rpkJ+m3nzhxIuethFsxOTnJxx9/rMmxRXuiPb21JW3ZTWef\nhURenb6kpITy8nLF0onaSHrk+UK0J9rTY1svQ+t1vM+BCxq3IRAINnITeKvQJyEQCAQCgUAgEAiK\nmsvAU+AZm+va58q/JymZnaqQ0AhcA0aAfwDUWhi1AL8GbMBj4F9p3F4V0A8MAEPADzRuT8IAWIGf\n56G9CeDRentS7mst2zMBfwc8IdmnvRq2d5ykXdIrQPKa0frzKygGYBQ4DJSTvHhPqtzGOaAHpdP/\nCPgf13//U5LqumqwF+he/70WGCZpj1btAUhTvWXAl8CbGrcH8K+BqySlyNG4vXGSTpCKlu39B+C/\nXv+9DKjXuD2JUmCW5I0jH+0VjNeB/y/l7++uv9TmMEqnfwpIgdN71//Wgp+RVO3NR3tG4C7QpXF7\nB4HrwNd5fqfXsr1xoOmFMq3aqwfGNinPx+f3T4BbeWzvpWi59/4AkKobNLVepjVmkkN+1n+aX1I3\nWw6THGH0a9xeKckRkovnjxZatvdvgT8BUndOadneGskvmXvAP9e4vSOAG/hr4AHw74AaDdtL5Z8C\nP1n/PR/tvRQtnX7L0L488tIQwyypBX4K/DEQ1Li9BMlHioPAeZJ3YK3aexeYJ/n8udX+DbXt+x2S\nX56/C/wLko9rWrVXBnwN+D/Wfy6xceSpxfVSAbwH/KdN/qdFe9uipdNPk3yGkbCQvNtrjYvksAlg\nH8kLWS3KSTr835Ac3mvdnkQA+CVwRsP23gB+j+SQ+yfAN0jaqaV9UrI3N/Ap8JqG7U2tv+6u//13\nJJ1/TqP2JH4XuE/SRsjP9fJStHT6e8ArJIfCFcCHPJ8c0pK/B/5w/fc/5Llz5koJ8FckZ33/Ig/t\nNfN8ZrcaeJvkXVir9v4NyS/mIySHo/8I/DMN2zMCUv6qGpLPvYMatjdH8nHz2Prfl0g+Lv1co/Yk\n/oDnQ3vQzj7d8LskZ7lHgT/T4Pg/AWaAFZIf6HdIzgZfR/0lkTdJDrcHeL4Mc1nD9k6RfPYcILms\n9Sfr5Vq1l8oFnn9Ba9XeEZK2DZBcApWuDy3tO03yTv8Q+M8kJ/e0bK8G8PD8yw2N2xMIBAKBQCAQ\nCAQCgUAgEAgEAoFAIBAIBAKBQCAQCASF5P8HUQFGXKIiTvUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e5b94d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pp = op.process_observation(initial_state )\n",
    "print pp.shape\n",
    "# N.B. matplotlib.pyplot.imshow() needs a 2D array, or a 3D array with the third dimension being of shape 3 or 4!\n",
    "# let's then remove the channel info for the plot\n",
    "imshow(np.reshape(pp, (pp.shape[0], pp.shape[1])),\n",
    "       cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent input shape (80, 80)\n",
      "Number of outputs 9\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute_2 (Permute)          (None, 80, 80, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_70 (Conv2D)           (None, 78, 78, 32)        1184      \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 78, 78, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_52 (MaxPooling (None, 39, 39, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_71 (Conv2D)           (None, 37, 37, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_53 (MaxPooling (None, 18, 18, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_72 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_54 (MaxPooling (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_69 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 9)                 1161      \n",
      "=================================================================\n",
      "Total params: 4,453,737\n",
      "Trainable params: 4,453,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/keras-rl/keras-rl/issues/113\n",
    "window_length = 4\n",
    "agent_state_shape = resized_state_shape\n",
    "print \"Agent input shape\", agent_state_shape\n",
    "print \"Number of outputs\", num_actions\n",
    "\n",
    "a = Agent(\"DQN\", \n",
    "          env_id,\n",
    "          agent_state_shape, \n",
    "          window_length,\n",
    "          num_actions, \n",
    "          Adam(lr=1e-3), metrics=['mae'],\n",
    "          preprocessor=op)\n",
    "a.__print_model_summary__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000000 steps ...\n",
      "     433/5000000: episode: 1, duration: 3.285s, episode steps: 433, steps per second: 132, episode reward: 24.000, mean reward: 0.055 [0.000, 1.000], mean action: 3.630 [0.000, 8.000], mean observation: 87.394 [54.000, 155.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "     904/5000000: episode: 2, duration: 2.800s, episode steps: 471, steps per second: 168, episode reward: 28.000, mean reward: 0.059 [0.000, 1.000], mean action: 3.919 [0.000, 8.000], mean observation: 87.416 [54.000, 155.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    1385/5000000: episode: 3, duration: 2.879s, episode steps: 481, steps per second: 167, episode reward: 26.000, mean reward: 0.054 [0.000, 1.000], mean action: 4.137 [0.000, 8.000], mean observation: 87.411 [54.000, 155.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    1854/5000000: episode: 4, duration: 3.015s, episode steps: 469, steps per second: 156, episode reward: 20.000, mean reward: 0.043 [0.000, 1.000], mean action: 3.994 [0.000, 8.000], mean observation: 87.472 [54.000, 155.000], loss: --, mean_absolute_error: --, mean_q: --, mean_eps: --\n",
      "    2367/5000000: episode: 5, duration: 176.683s, episode steps: 513, steps per second: 3, episode reward: 22.000, mean reward: 0.043 [0.000, 1.000], mean action: 3.908 [0.000, 8.000], mean observation: 87.505 [54.000, 155.000], loss: 0.024854, mean_absolute_error: 0.105426, mean_q: 0.132856, mean_eps: 0.998035\n",
      "    2818/5000000: episode: 6, duration: 207.560s, episode steps: 451, steps per second: 2, episode reward: 21.000, mean reward: 0.047 [0.000, 1.000], mean action: 4.113 [0.000, 8.000], mean observation: 87.499 [54.000, 155.000], loss: 0.025467, mean_absolute_error: 0.105504, mean_q: 0.133023, mean_eps: 0.997667\n",
      "    3285/5000000: episode: 7, duration: 217.334s, episode steps: 467, steps per second: 2, episode reward: 21.000, mean reward: 0.045 [0.000, 1.000], mean action: 4.028 [0.000, 8.000], mean observation: 87.465 [54.000, 155.000], loss: 0.024656, mean_absolute_error: 0.105423, mean_q: 0.128945, mean_eps: 0.997254\n",
      "    3730/5000000: episode: 8, duration: 201.350s, episode steps: 445, steps per second: 2, episode reward: 26.000, mean reward: 0.058 [0.000, 1.000], mean action: 4.110 [0.000, 8.000], mean observation: 87.445 [54.000, 155.000], loss: 0.026511, mean_absolute_error: 0.105896, mean_q: 0.131351, mean_eps: 0.996844\n",
      "    4263/5000000: episode: 9, duration: 238.271s, episode steps: 533, steps per second: 2, episode reward: 21.000, mean reward: 0.039 [0.000, 1.000], mean action: 4.073 [0.000, 8.000], mean observation: 87.481 [54.000, 155.000], loss: 0.024206, mean_absolute_error: 0.105104, mean_q: 0.124177, mean_eps: 0.996404\n",
      "    4674/5000000: episode: 10, duration: 195.142s, episode steps: 411, steps per second: 2, episode reward: 17.000, mean reward: 0.041 [0.000, 1.000], mean action: 4.182 [0.000, 8.000], mean observation: 87.450 [54.000, 155.000], loss: 0.023695, mean_absolute_error: 0.104788, mean_q: 0.121935, mean_eps: 0.995979\n",
      "    5169/5000000: episode: 11, duration: 224.107s, episode steps: 495, steps per second: 2, episode reward: 15.000, mean reward: 0.030 [0.000, 1.000], mean action: 3.911 [0.000, 8.000], mean observation: 87.486 [54.000, 155.000], loss: 0.023583, mean_absolute_error: 0.104712, mean_q: 0.122055, mean_eps: 0.995571\n",
      "    5706/5000000: episode: 12, duration: 244.513s, episode steps: 537, steps per second: 2, episode reward: 34.000, mean reward: 0.063 [0.000, 1.000], mean action: 3.942 [0.000, 8.000], mean observation: 87.352 [54.000, 155.000], loss: 0.023277, mean_absolute_error: 0.104821, mean_q: 0.123480, mean_eps: 0.995107\n",
      "    6123/5000000: episode: 13, duration: 190.870s, episode steps: 417, steps per second: 2, episode reward: 17.000, mean reward: 0.041 [0.000, 1.000], mean action: 3.808 [0.000, 8.000], mean observation: 87.485 [54.000, 155.000], loss: 0.023837, mean_absolute_error: 0.104941, mean_q: 0.130592, mean_eps: 0.994677\n",
      "    6400/5000000: episode: 14, duration: 121.745s, episode steps: 277, steps per second: 2, episode reward: 12.000, mean reward: 0.043 [0.000, 1.000], mean action: 4.040 [0.000, 8.000], mean observation: 87.531 [54.000, 155.000], loss: 0.023159, mean_absolute_error: 0.104605, mean_q: 0.123759, mean_eps: 0.994365\n",
      "    6941/5000000: episode: 15, duration: 251.388s, episode steps: 541, steps per second: 2, episode reward: 23.000, mean reward: 0.043 [0.000, 1.000], mean action: 3.765 [0.000, 8.000], mean observation: 87.444 [54.000, 155.000], loss: 0.024128, mean_absolute_error: 0.105034, mean_q: 0.125018, mean_eps: 0.993997\n",
      "    7382/5000000: episode: 16, duration: 221.053s, episode steps: 441, steps per second: 2, episode reward: 22.000, mean reward: 0.050 [0.000, 1.000], mean action: 4.102 [0.000, 8.000], mean observation: 87.496 [54.000, 155.000], loss: 0.022623, mean_absolute_error: 0.104933, mean_q: 0.126224, mean_eps: 0.993555\n",
      "    7843/5000000: episode: 17, duration: 211.185s, episode steps: 461, steps per second: 2, episode reward: 23.000, mean reward: 0.050 [0.000, 1.000], mean action: 3.863 [0.000, 8.000], mean observation: 87.455 [54.000, 155.000], loss: 0.022950, mean_absolute_error: 0.104897, mean_q: 0.126633, mean_eps: 0.993149\n",
      "    8380/5000000: episode: 18, duration: 243.354s, episode steps: 537, steps per second: 2, episode reward: 33.000, mean reward: 0.061 [0.000, 1.000], mean action: 3.946 [0.000, 8.000], mean observation: 87.396 [54.000, 155.000], loss: 0.024840, mean_absolute_error: 0.105246, mean_q: 0.125931, mean_eps: 0.992700\n",
      "    8855/5000000: episode: 19, duration: 218.149s, episode steps: 475, steps per second: 2, episode reward: 18.000, mean reward: 0.038 [0.000, 1.000], mean action: 4.000 [0.000, 8.000], mean observation: 87.463 [54.000, 155.000], loss: 0.023106, mean_absolute_error: 0.104940, mean_q: 0.127793, mean_eps: 0.992245\n",
      "    9432/5000000: episode: 20, duration: 288.256s, episode steps: 577, steps per second: 2, episode reward: 33.000, mean reward: 0.057 [0.000, 1.000], mean action: 3.792 [0.000, 8.000], mean observation: 87.323 [54.000, 214.000], loss: 0.023722, mean_absolute_error: 0.104940, mean_q: 0.124306, mean_eps: 0.991771\n",
      "    9827/5000000: episode: 21, duration: 187.889s, episode steps: 395, steps per second: 2, episode reward: 18.000, mean reward: 0.046 [0.000, 1.000], mean action: 4.192 [0.000, 8.000], mean observation: 87.476 [54.000, 155.000], loss: 0.024126, mean_absolute_error: 0.104948, mean_q: 0.124933, mean_eps: 0.991334\n",
      "   10310/5000000: episode: 22, duration: 225.614s, episode steps: 483, steps per second: 2, episode reward: 24.000, mean reward: 0.050 [0.000, 1.000], mean action: 3.791 [0.000, 8.000], mean observation: 87.430 [54.000, 155.000], loss: 0.024189, mean_absolute_error: 0.105752, mean_q: 0.130833, mean_eps: 0.990939\n",
      "   10847/5000000: episode: 23, duration: 244.447s, episode steps: 537, steps per second: 2, episode reward: 29.000, mean reward: 0.054 [0.000, 1.000], mean action: 3.844 [0.000, 8.000], mean observation: 87.304 [54.000, 155.000], loss: 0.024753, mean_absolute_error: 0.106125, mean_q: 0.131303, mean_eps: 0.990480\n",
      "   11358/5000000: episode: 24, duration: 240.000s, episode steps: 511, steps per second: 2, episode reward: 25.000, mean reward: 0.049 [0.000, 1.000], mean action: 3.822 [0.000, 8.000], mean observation: 87.468 [54.000, 155.000], loss: 0.025053, mean_absolute_error: 0.106112, mean_q: 0.124815, mean_eps: 0.990008\n",
      "   11839/5000000: episode: 25, duration: 219.262s, episode steps: 481, steps per second: 2, episode reward: 14.000, mean reward: 0.029 [0.000, 1.000], mean action: 4.048 [0.000, 8.000], mean observation: 87.477 [54.000, 155.000], loss: 0.025516, mean_absolute_error: 0.106187, mean_q: 0.123374, mean_eps: 0.989562\n",
      "   12318/5000000: episode: 26, duration: 215.063s, episode steps: 479, steps per second: 2, episode reward: 18.000, mean reward: 0.038 [0.000, 1.000], mean action: 3.756 [0.000, 8.000], mean observation: 87.519 [54.000, 155.000], loss: 0.024235, mean_absolute_error: 0.105910, mean_q: 0.124744, mean_eps: 0.989130\n",
      "   12829/5000000: episode: 27, duration: 238.489s, episode steps: 511, steps per second: 2, episode reward: 24.000, mean reward: 0.047 [0.000, 1.000], mean action: 4.000 [0.000, 8.000], mean observation: 87.476 [54.000, 155.000], loss: 0.024575, mean_absolute_error: 0.105972, mean_q: 0.124446, mean_eps: 0.988684\n",
      "   13252/5000000: episode: 28, duration: 201.664s, episode steps: 423, steps per second: 2, episode reward: 21.000, mean reward: 0.050 [0.000, 1.000], mean action: 4.064 [0.000, 8.000], mean observation: 87.454 [54.000, 155.000], loss: 0.024626, mean_absolute_error: 0.105991, mean_q: 0.127309, mean_eps: 0.988264\n",
      "   13759/5000000: episode: 29, duration: 245.409s, episode steps: 507, steps per second: 2, episode reward: 30.000, mean reward: 0.059 [0.000, 1.000], mean action: 3.874 [0.000, 8.000], mean observation: 87.347 [54.000, 155.000], loss: 0.023862, mean_absolute_error: 0.105813, mean_q: 0.129154, mean_eps: 0.987845\n",
      "   14150/5000000: episode: 30, duration: 907.698s, episode steps: 391, steps per second: 0, episode reward: 23.000, mean reward: 0.059 [0.000, 1.000], mean action: 3.923 [0.000, 8.000], mean observation: 87.418 [54.000, 155.000], loss: 0.026096, mean_absolute_error: 0.106312, mean_q: 0.129652, mean_eps: 0.987441\n",
      "   14587/5000000: episode: 31, duration: 197.296s, episode steps: 437, steps per second: 2, episode reward: 27.000, mean reward: 0.062 [0.000, 1.000], mean action: 4.140 [0.000, 8.000], mean observation: 87.353 [54.000, 155.000], loss: 0.025401, mean_absolute_error: 0.106154, mean_q: 0.129715, mean_eps: 0.987069\n",
      "   15038/5000000: episode: 32, duration: 215.341s, episode steps: 451, steps per second: 2, episode reward: 31.000, mean reward: 0.069 [0.000, 1.000], mean action: 3.902 [0.000, 8.000], mean observation: 87.437 [54.000, 155.000], loss: 0.025501, mean_absolute_error: 0.106229, mean_q: 0.131181, mean_eps: 0.986669\n",
      "   15487/5000000: episode: 33, duration: 201.426s, episode steps: 449, steps per second: 2, episode reward: 22.000, mean reward: 0.049 [0.000, 1.000], mean action: 3.944 [0.000, 8.000], mean observation: 87.459 [54.000, 155.000], loss: 0.023750, mean_absolute_error: 0.105882, mean_q: 0.132466, mean_eps: 0.986264\n",
      "   16000/5000000: episode: 34, duration: 223.371s, episode steps: 513, steps per second: 2, episode reward: 19.000, mean reward: 0.037 [0.000, 1.000], mean action: 4.117 [0.000, 8.000], mean observation: 87.568 [54.000, 155.000], loss: 0.024527, mean_absolute_error: 0.106042, mean_q: 0.132787, mean_eps: 0.985831\n",
      "   16519/5000000: episode: 35, duration: 246.894s, episode steps: 519, steps per second: 2, episode reward: 18.000, mean reward: 0.035 [0.000, 1.000], mean action: 3.944 [0.000, 8.000], mean observation: 87.497 [54.000, 155.000], loss: 0.025969, mean_absolute_error: 0.106263, mean_q: 0.129841, mean_eps: 0.985367\n",
      "   16978/5000000: episode: 36, duration: 199.471s, episode steps: 459, steps per second: 2, episode reward: 27.000, mean reward: 0.059 [0.000, 1.000], mean action: 3.743 [0.000, 8.000], mean observation: 87.362 [54.000, 155.000], loss: 0.026561, mean_absolute_error: 0.106359, mean_q: 0.129189, mean_eps: 0.984927\n",
      "   17667/5000000: episode: 37, duration: 312.417s, episode steps: 689, steps per second: 2, episode reward: 34.000, mean reward: 0.049 [0.000, 1.000], mean action: 3.927 [0.000, 8.000], mean observation: 87.303 [54.000, 155.000], loss: 0.025272, mean_absolute_error: 0.106105, mean_q: 0.128582, mean_eps: 0.984410\n",
      "   18184/5000000: episode: 38, duration: 233.183s, episode steps: 517, steps per second: 2, episode reward: 22.000, mean reward: 0.043 [0.000, 1.000], mean action: 4.114 [0.000, 8.000], mean observation: 87.468 [54.000, 155.000], loss: 0.026356, mean_absolute_error: 0.106377, mean_q: 0.131330, mean_eps: 0.983868\n",
      "   18619/5000000: episode: 39, duration: 197.875s, episode steps: 435, steps per second: 2, episode reward: 23.000, mean reward: 0.053 [0.000, 1.000], mean action: 4.175 [0.000, 8.000], mean observation: 87.450 [54.000, 155.000], loss: 0.025224, mean_absolute_error: 0.106178, mean_q: 0.132107, mean_eps: 0.983439\n",
      "   19068/5000000: episode: 40, duration: 195.721s, episode steps: 449, steps per second: 2, episode reward: 25.000, mean reward: 0.056 [0.000, 1.000], mean action: 3.886 [0.000, 8.000], mean observation: 87.467 [54.000, 155.000], loss: 0.025437, mean_absolute_error: 0.106267, mean_q: 0.133905, mean_eps: 0.983041\n",
      "   19493/5000000: episode: 41, duration: 192.404s, episode steps: 425, steps per second: 2, episode reward: 19.000, mean reward: 0.045 [0.000, 1.000], mean action: 4.134 [0.000, 8.000], mean observation: 87.494 [54.000, 155.000], loss: 0.025570, mean_absolute_error: 0.106179, mean_q: 0.128404, mean_eps: 0.982648\n",
      "   19882/5000000: episode: 42, duration: 176.268s, episode steps: 389, steps per second: 2, episode reward: 17.000, mean reward: 0.044 [0.000, 1.000], mean action: 3.920 [0.000, 8.000], mean observation: 87.511 [54.000, 155.000], loss: 0.025467, mean_absolute_error: 0.106147, mean_q: 0.127727, mean_eps: 0.982282\n",
      "   20391/5000000: episode: 43, duration: 219.931s, episode steps: 509, steps per second: 2, episode reward: 18.000, mean reward: 0.035 [0.000, 1.000], mean action: 4.061 [0.000, 8.000], mean observation: 87.478 [54.000, 155.000], loss: 0.024869, mean_absolute_error: 0.105813, mean_q: 0.127268, mean_eps: 0.981878\n",
      "   20868/5000000: episode: 44, duration: 270.231s, episode steps: 477, steps per second: 2, episode reward: 23.000, mean reward: 0.048 [0.000, 1.000], mean action: 4.036 [0.000, 8.000], mean observation: 87.427 [54.000, 155.000], loss: 0.024195, mean_absolute_error: 0.105639, mean_q: 0.127294, mean_eps: 0.981434\n",
      "   21335/5000000: episode: 45, duration: 276.079s, episode steps: 467, steps per second: 2, episode reward: 19.000, mean reward: 0.041 [0.000, 1.000], mean action: 3.916 [0.000, 8.000], mean observation: 87.490 [54.000, 155.000], loss: 0.023563, mean_absolute_error: 0.105552, mean_q: 0.129366, mean_eps: 0.981009\n",
      "   21776/5000000: episode: 46, duration: 260.447s, episode steps: 441, steps per second: 2, episode reward: 28.000, mean reward: 0.063 [0.000, 1.000], mean action: 3.905 [0.000, 8.000], mean observation: 87.357 [54.000, 155.000], loss: 0.026031, mean_absolute_error: 0.106074, mean_q: 0.128480, mean_eps: 0.980600\n",
      "   22231/5000000: episode: 47, duration: 269.155s, episode steps: 455, steps per second: 2, episode reward: 18.000, mean reward: 0.040 [0.000, 1.000], mean action: 3.756 [0.000, 8.000], mean observation: 87.511 [54.000, 155.000], loss: 0.023968, mean_absolute_error: 0.105696, mean_q: 0.132541, mean_eps: 0.980197\n",
      "   22724/5000000: episode: 48, duration: 291.399s, episode steps: 493, steps per second: 2, episode reward: 22.000, mean reward: 0.045 [0.000, 1.000], mean action: 4.089 [0.000, 8.000], mean observation: 87.459 [54.000, 155.000], loss: 0.024081, mean_absolute_error: 0.105835, mean_q: 0.137272, mean_eps: 0.979771\n",
      "   23271/5000000: episode: 49, duration: 256.339s, episode steps: 547, steps per second: 2, episode reward: 20.000, mean reward: 0.037 [0.000, 1.000], mean action: 3.867 [0.000, 8.000], mean observation: 87.499 [54.000, 155.000], loss: 0.024504, mean_absolute_error: 0.105893, mean_q: 0.136903, mean_eps: 0.979303\n",
      "   24142/5000000: episode: 50, duration: 433.512s, episode steps: 871, steps per second: 2, episode reward: 48.000, mean reward: 0.055 [0.000, 1.000], mean action: 3.890 [0.000, 8.000], mean observation: 87.263 [54.000, 155.000], loss: 0.025583, mean_absolute_error: 0.106133, mean_q: 0.135671, mean_eps: 0.978665\n",
      "   24573/5000000: episode: 51, duration: 185.638s, episode steps: 431, steps per second: 2, episode reward: 26.000, mean reward: 0.060 [0.000, 1.000], mean action: 3.993 [0.000, 8.000], mean observation: 87.419 [54.000, 155.000], loss: 0.025483, mean_absolute_error: 0.106120, mean_q: 0.135350, mean_eps: 0.978079\n",
      "   25120/5000000: episode: 52, duration: 235.308s, episode steps: 547, steps per second: 2, episode reward: 33.000, mean reward: 0.060 [0.000, 1.000], mean action: 4.188 [0.000, 8.000], mean observation: 87.266 [54.000, 155.000], loss: 0.024924, mean_absolute_error: 0.105990, mean_q: 0.135460, mean_eps: 0.977639\n",
      "   25593/5000000: episode: 53, duration: 277.324s, episode steps: 473, steps per second: 2, episode reward: 18.000, mean reward: 0.038 [0.000, 1.000], mean action: 4.051 [0.000, 8.000], mean observation: 87.504 [54.000, 155.000], loss: 0.024657, mean_absolute_error: 0.105919, mean_q: 0.135094, mean_eps: 0.977180\n",
      "   25848/5000000: episode: 54, duration: 150.995s, episode steps: 255, steps per second: 2, episode reward: 11.000, mean reward: 0.043 [0.000, 1.000], mean action: 4.090 [0.000, 8.000], mean observation: 87.541 [54.000, 155.000], loss: 0.025168, mean_absolute_error: 0.106036, mean_q: 0.134779, mean_eps: 0.976852\n",
      "   26363/5000000: episode: 55, duration: 304.828s, episode steps: 515, steps per second: 2, episode reward: 24.000, mean reward: 0.047 [0.000, 1.000], mean action: 3.895 [0.000, 8.000], mean observation: 87.480 [54.000, 155.000], loss: 0.024481, mean_absolute_error: 0.105910, mean_q: 0.137311, mean_eps: 0.976506\n",
      "   26868/5000000: episode: 56, duration: 298.229s, episode steps: 505, steps per second: 2, episode reward: 19.000, mean reward: 0.038 [0.000, 1.000], mean action: 3.923 [0.000, 8.000], mean observation: 87.552 [54.000, 155.000], loss: 0.026276, mean_absolute_error: 0.106241, mean_q: 0.134930, mean_eps: 0.976047\n",
      "   27357/5000000: episode: 57, duration: 288.189s, episode steps: 489, steps per second: 2, episode reward: 25.000, mean reward: 0.051 [0.000, 1.000], mean action: 4.025 [0.000, 8.000], mean observation: 87.436 [54.000, 155.000], loss: 0.023930, mean_absolute_error: 0.105717, mean_q: 0.132936, mean_eps: 0.975599\n",
      "   27862/5000000: episode: 58, duration: 268.225s, episode steps: 505, steps per second: 2, episode reward: 27.000, mean reward: 0.053 [0.000, 1.000], mean action: 3.958 [0.000, 8.000], mean observation: 87.511 [54.000, 155.000], loss: 0.025797, mean_absolute_error: 0.106126, mean_q: 0.135237, mean_eps: 0.975152\n",
      "   28333/5000000: episode: 59, duration: 201.077s, episode steps: 471, steps per second: 2, episode reward: 23.000, mean reward: 0.049 [0.000, 1.000], mean action: 3.945 [0.000, 8.000], mean observation: 87.471 [54.000, 155.000], loss: 0.027170, mean_absolute_error: 0.106510, mean_q: 0.136997, mean_eps: 0.974713\n",
      "   28810/5000000: episode: 60, duration: 208.913s, episode steps: 477, steps per second: 2, episode reward: 27.000, mean reward: 0.057 [0.000, 1.000], mean action: 3.935 [0.000, 8.000], mean observation: 87.441 [54.000, 155.000], loss: 0.024274, mean_absolute_error: 0.105815, mean_q: 0.135459, mean_eps: 0.974286\n",
      "   29213/5000000: episode: 61, duration: 172.080s, episode steps: 403, steps per second: 2, episode reward: 17.000, mean reward: 0.042 [0.000, 1.000], mean action: 3.831 [0.000, 8.000], mean observation: 87.485 [54.000, 155.000], loss: 0.023644, mean_absolute_error: 0.105625, mean_q: 0.133179, mean_eps: 0.973890\n",
      "   29634/5000000: episode: 62, duration: 210.706s, episode steps: 421, steps per second: 2, episode reward: 14.000, mean reward: 0.033 [0.000, 1.000], mean action: 4.010 [0.000, 8.000], mean observation: 87.510 [54.000, 155.000], loss: 0.025789, mean_absolute_error: 0.106053, mean_q: 0.130070, mean_eps: 0.973519\n",
      "   30023/5000000: episode: 63, duration: 230.213s, episode steps: 389, steps per second: 2, episode reward: 14.000, mean reward: 0.036 [0.000, 1.000], mean action: 3.787 [0.000, 8.000], mean observation: 87.535 [54.000, 155.000], loss: 0.025263, mean_absolute_error: 0.105917, mean_q: 0.130225, mean_eps: 0.973155\n",
      "   30520/5000000: episode: 64, duration: 292.789s, episode steps: 497, steps per second: 2, episode reward: 22.000, mean reward: 0.044 [0.000, 1.000], mean action: 4.197 [0.000, 8.000], mean observation: 87.457 [54.000, 155.000], loss: 0.024543, mean_absolute_error: 0.106072, mean_q: 0.131378, mean_eps: 0.972756\n",
      "   31431/5000000: episode: 65, duration: 535.262s, episode steps: 911, steps per second: 2, episode reward: 55.000, mean reward: 0.060 [0.000, 1.000], mean action: 3.980 [0.000, 8.000], mean observation: 87.138 [54.000, 214.000], loss: 0.024775, mean_absolute_error: 0.106145, mean_q: 0.132383, mean_eps: 0.972123\n",
      "   31934/5000000: episode: 66, duration: 295.559s, episode steps: 503, steps per second: 2, episode reward: 20.000, mean reward: 0.040 [0.000, 1.000], mean action: 3.857 [0.000, 8.000], mean observation: 87.498 [54.000, 155.000], loss: 0.024837, mean_absolute_error: 0.106142, mean_q: 0.131599, mean_eps: 0.971486\n",
      "   32445/5000000: episode: 67, duration: 229.119s, episode steps: 511, steps per second: 2, episode reward: 19.000, mean reward: 0.037 [0.000, 1.000], mean action: 3.867 [0.000, 8.000], mean observation: 87.542 [54.000, 155.000], loss: 0.024865, mean_absolute_error: 0.106148, mean_q: 0.131191, mean_eps: 0.971030\n",
      "   32958/5000000: episode: 68, duration: 218.320s, episode steps: 513, steps per second: 2, episode reward: 26.000, mean reward: 0.051 [0.000, 1.000], mean action: 4.234 [0.000, 8.000], mean observation: 87.414 [54.000, 155.000], loss: 0.024734, mean_absolute_error: 0.106114, mean_q: 0.131739, mean_eps: 0.970569\n",
      "   33377/5000000: episode: 69, duration: 178.338s, episode steps: 419, steps per second: 2, episode reward: 17.000, mean reward: 0.041 [0.000, 1.000], mean action: 3.916 [0.000, 8.000], mean observation: 87.508 [54.000, 155.000], loss: 0.025757, mean_absolute_error: 0.106381, mean_q: 0.132450, mean_eps: 0.970150\n",
      "   33802/5000000: episode: 70, duration: 181.160s, episode steps: 425, steps per second: 2, episode reward: 18.000, mean reward: 0.042 [0.000, 1.000], mean action: 4.035 [0.000, 8.000], mean observation: 87.456 [54.000, 155.000], loss: 0.024646, mean_absolute_error: 0.106112, mean_q: 0.129444, mean_eps: 0.969770\n",
      "   34283/5000000: episode: 71, duration: 205.271s, episode steps: 481, steps per second: 2, episode reward: 21.000, mean reward: 0.044 [0.000, 1.000], mean action: 3.956 [0.000, 8.000], mean observation: 87.481 [54.000, 155.000], loss: 0.025548, mean_absolute_error: 0.106282, mean_q: 0.131215, mean_eps: 0.969362\n",
      "   34838/5000000: episode: 72, duration: 236.951s, episode steps: 555, steps per second: 2, episode reward: 31.000, mean reward: 0.056 [0.000, 1.000], mean action: 4.079 [0.000, 8.000], mean observation: 87.395 [54.000, 155.000], loss: 0.025363, mean_absolute_error: 0.106273, mean_q: 0.133060, mean_eps: 0.968896\n",
      "   35263/5000000: episode: 73, duration: 182.340s, episode steps: 425, steps per second: 2, episode reward: 22.000, mean reward: 0.052 [0.000, 1.000], mean action: 3.706 [0.000, 8.000], mean observation: 87.519 [54.000, 155.000], loss: 0.024602, mean_absolute_error: 0.106067, mean_q: 0.131423, mean_eps: 0.968455\n",
      "   35754/5000000: episode: 74, duration: 212.342s, episode steps: 491, steps per second: 2, episode reward: 24.000, mean reward: 0.049 [0.000, 1.000], mean action: 3.857 [0.000, 8.000], mean observation: 87.473 [54.000, 155.000], loss: 0.026894, mean_absolute_error: 0.106580, mean_q: 0.131746, mean_eps: 0.968043\n",
      "   36219/5000000: episode: 75, duration: 200.371s, episode steps: 465, steps per second: 2, episode reward: 30.000, mean reward: 0.065 [0.000, 1.000], mean action: 3.772 [0.000, 8.000], mean observation: 87.370 [54.000, 155.000], loss: 0.024219, mean_absolute_error: 0.105986, mean_q: 0.131966, mean_eps: 0.967613\n",
      "   36760/5000000: episode: 76, duration: 233.631s, episode steps: 541, steps per second: 2, episode reward: 25.000, mean reward: 0.046 [0.000, 1.000], mean action: 3.980 [0.000, 8.000], mean observation: 87.457 [54.000, 155.000], loss: 0.025698, mean_absolute_error: 0.106312, mean_q: 0.129356, mean_eps: 0.967160\n",
      "   37185/5000000: episode: 77, duration: 183.855s, episode steps: 425, steps per second: 2, episode reward: 17.000, mean reward: 0.040 [0.000, 1.000], mean action: 4.301 [0.000, 8.000], mean observation: 87.586 [54.000, 155.000], loss: 0.024384, mean_absolute_error: 0.106052, mean_q: 0.132688, mean_eps: 0.966725\n",
      "   37690/5000000: episode: 78, duration: 220.120s, episode steps: 505, steps per second: 2, episode reward: 30.000, mean reward: 0.059 [0.000, 1.000], mean action: 4.156 [0.000, 8.000], mean observation: 87.410 [54.000, 155.000], loss: 0.025677, mean_absolute_error: 0.106392, mean_q: 0.135278, mean_eps: 0.966307\n",
      "   38223/5000000: episode: 79, duration: 230.778s, episode steps: 533, steps per second: 2, episode reward: 28.000, mean reward: 0.053 [0.000, 1.000], mean action: 4.090 [0.000, 8.000], mean observation: 87.375 [54.000, 155.000], loss: 0.026375, mean_absolute_error: 0.106589, mean_q: 0.136940, mean_eps: 0.965840\n",
      "   38700/5000000: episode: 80, duration: 206.252s, episode steps: 477, steps per second: 2, episode reward: 18.000, mean reward: 0.038 [0.000, 1.000], mean action: 3.933 [0.000, 8.000], mean observation: 87.511 [54.000, 155.000], loss: 0.025388, mean_absolute_error: 0.106333, mean_q: 0.135271, mean_eps: 0.965385\n",
      "   39205/5000000: episode: 81, duration: 217.871s, episode steps: 505, steps per second: 2, episode reward: 26.000, mean reward: 0.051 [0.000, 1.000], mean action: 3.996 [0.000, 8.000], mean observation: 87.392 [54.000, 155.000], loss: 0.024100, mean_absolute_error: 0.106034, mean_q: 0.133436, mean_eps: 0.964943\n",
      "   39594/5000000: episode: 82, duration: 168.465s, episode steps: 389, steps per second: 2, episode reward: 30.000, mean reward: 0.077 [0.000, 1.000], mean action: 3.933 [0.000, 8.000], mean observation: 87.357 [54.000, 155.000], loss: 0.024946, mean_absolute_error: 0.106157, mean_q: 0.131931, mean_eps: 0.964541\n",
      "   40121/5000000: episode: 83, duration: 227.130s, episode steps: 527, steps per second: 2, episode reward: 26.000, mean reward: 0.049 [0.000, 1.000], mean action: 4.008 [0.000, 8.000], mean observation: 87.367 [54.000, 155.000], loss: 0.025365, mean_absolute_error: 0.106356, mean_q: 0.134165, mean_eps: 0.964129\n",
      "   40638/5000000: episode: 84, duration: 223.696s, episode steps: 517, steps per second: 2, episode reward: 20.000, mean reward: 0.039 [0.000, 1.000], mean action: 4.116 [0.000, 8.000], mean observation: 87.519 [54.000, 155.000], loss: 0.024573, mean_absolute_error: 0.106382, mean_q: 0.133805, mean_eps: 0.963659\n",
      "   41207/5000000: episode: 85, duration: 245.107s, episode steps: 569, steps per second: 2, episode reward: 31.000, mean reward: 0.054 [0.000, 1.000], mean action: 4.049 [0.000, 8.000], mean observation: 87.306 [54.000, 155.000], loss: 0.026694, mean_absolute_error: 0.106824, mean_q: 0.134928, mean_eps: 0.963170\n",
      "   41770/5000000: episode: 86, duration: 242.850s, episode steps: 563, steps per second: 2, episode reward: 37.000, mean reward: 0.066 [0.000, 1.000], mean action: 3.925 [0.000, 8.000], mean observation: 87.263 [54.000, 155.000], loss: 0.025519, mean_absolute_error: 0.106574, mean_q: 0.132283, mean_eps: 0.962661\n",
      "   42407/5000000: episode: 87, duration: 274.753s, episode steps: 637, steps per second: 2, episode reward: 36.000, mean reward: 0.057 [0.000, 1.000], mean action: 3.915 [0.000, 8.000], mean observation: 87.281 [54.000, 214.000], loss: 0.025959, mean_absolute_error: 0.106680, mean_q: 0.129167, mean_eps: 0.962121\n",
      "   42938/5000000: episode: 88, duration: 233.434s, episode steps: 531, steps per second: 2, episode reward: 25.000, mean reward: 0.047 [0.000, 1.000], mean action: 4.121 [0.000, 8.000], mean observation: 87.484 [54.000, 155.000], loss: 0.023968, mean_absolute_error: 0.106098, mean_q: 0.129569, mean_eps: 0.961595\n",
      "   43359/5000000: episode: 89, duration: 181.239s, episode steps: 421, steps per second: 2, episode reward: 13.000, mean reward: 0.031 [0.000, 1.000], mean action: 3.976 [0.000, 8.000], mean observation: 87.564 [54.000, 155.000], loss: 0.025223, mean_absolute_error: 0.106545, mean_q: 0.134590, mean_eps: 0.961167\n",
      "   43862/5000000: episode: 90, duration: 218.314s, episode steps: 503, steps per second: 2, episode reward: 29.000, mean reward: 0.058 [0.000, 1.000], mean action: 4.072 [0.000, 8.000], mean observation: 87.296 [54.000, 155.000], loss: 0.024450, mean_absolute_error: 0.106380, mean_q: 0.133883, mean_eps: 0.960751\n",
      "   44407/5000000: episode: 91, duration: 235.259s, episode steps: 545, steps per second: 2, episode reward: 25.000, mean reward: 0.046 [0.000, 1.000], mean action: 3.906 [0.000, 8.000], mean observation: 87.429 [54.000, 155.000], loss: 0.025761, mean_absolute_error: 0.106613, mean_q: 0.132817, mean_eps: 0.960279\n",
      "   44976/5000000: episode: 92, duration: 246.026s, episode steps: 569, steps per second: 2, episode reward: 30.000, mean reward: 0.053 [0.000, 1.000], mean action: 3.886 [0.000, 8.000], mean observation: 87.339 [54.000, 155.000], loss: 0.025374, mean_absolute_error: 0.106552, mean_q: 0.133106, mean_eps: 0.959778\n",
      "   45457/5000000: episode: 93, duration: 209.743s, episode steps: 481, steps per second: 2, episode reward: 22.000, mean reward: 0.046 [0.000, 1.000], mean action: 3.904 [0.000, 8.000], mean observation: 87.505 [54.000, 155.000], loss: 0.025587, mean_absolute_error: 0.106600, mean_q: 0.134783, mean_eps: 0.959306\n",
      "   45950/5000000: episode: 94, duration: 213.609s, episode steps: 493, steps per second: 2, episode reward: 25.000, mean reward: 0.051 [0.000, 1.000], mean action: 4.118 [0.000, 8.000], mean observation: 87.411 [54.000, 155.000], loss: 0.025508, mean_absolute_error: 0.106541, mean_q: 0.130136, mean_eps: 0.958867\n",
      "   46499/5000000: episode: 95, duration: 236.541s, episode steps: 549, steps per second: 2, episode reward: 24.000, mean reward: 0.044 [0.000, 1.000], mean action: 4.029 [0.000, 8.000], mean observation: 87.448 [54.000, 155.000], loss: 0.024695, mean_absolute_error: 0.106435, mean_q: 0.134047, mean_eps: 0.958398\n",
      "   46986/5000000: episode: 96, duration: 211.017s, episode steps: 487, steps per second: 2, episode reward: 24.000, mean reward: 0.049 [0.000, 1.000], mean action: 3.871 [0.000, 8.000], mean observation: 87.506 [54.000, 155.000], loss: 0.025187, mean_absolute_error: 0.106518, mean_q: 0.134862, mean_eps: 0.957932\n",
      "   47531/5000000: episode: 97, duration: 236.240s, episode steps: 545, steps per second: 2, episode reward: 22.000, mean reward: 0.040 [0.000, 1.000], mean action: 3.982 [0.000, 8.000], mean observation: 87.422 [54.000, 155.000], loss: 0.026452, mean_absolute_error: 0.106783, mean_q: 0.135129, mean_eps: 0.957468\n",
      "   47962/5000000: episode: 98, duration: 186.618s, episode steps: 431, steps per second: 2, episode reward: 21.000, mean reward: 0.049 [0.000, 1.000], mean action: 3.905 [0.000, 8.000], mean observation: 87.371 [54.000, 155.000], loss: 0.024832, mean_absolute_error: 0.106432, mean_q: 0.135139, mean_eps: 0.957029\n",
      "   48269/5000000: episode: 99, duration: 132.613s, episode steps: 307, steps per second: 2, episode reward: 8.000, mean reward: 0.026 [0.000, 1.000], mean action: 4.033 [0.000, 8.000], mean observation: 87.583 [54.000, 155.000], loss: 0.025738, mean_absolute_error: 0.106620, mean_q: 0.134533, mean_eps: 0.956697\n",
      "   48656/5000000: episode: 100, duration: 167.843s, episode steps: 387, steps per second: 2, episode reward: 21.000, mean reward: 0.054 [0.000, 1.000], mean action: 3.992 [0.000, 8.000], mean observation: 87.418 [54.000, 155.000], loss: 0.024856, mean_absolute_error: 0.106424, mean_q: 0.132970, mean_eps: 0.956384\n",
      "   49231/5000000: episode: 101, duration: 248.137s, episode steps: 575, steps per second: 2, episode reward: 26.000, mean reward: 0.045 [0.000, 1.000], mean action: 3.908 [0.000, 8.000], mean observation: 87.389 [54.000, 155.000], loss: 0.025206, mean_absolute_error: 0.106482, mean_q: 0.131416, mean_eps: 0.955951\n",
      "   49790/5000000: episode: 102, duration: 240.665s, episode steps: 559, steps per second: 2, episode reward: 32.000, mean reward: 0.057 [0.000, 1.000], mean action: 3.970 [0.000, 8.000], mean observation: 87.292 [54.000, 155.000], loss: 0.025479, mean_absolute_error: 0.106529, mean_q: 0.134687, mean_eps: 0.955441\n",
      "   50357/5000000: episode: 103, duration: 244.177s, episode steps: 567, steps per second: 2, episode reward: 28.000, mean reward: 0.049 [0.000, 1.000], mean action: 4.030 [0.000, 8.000], mean observation: 87.435 [54.000, 155.000], loss: 0.026231, mean_absolute_error: 0.106676, mean_q: 0.134401, mean_eps: 0.954934\n",
      "   50628/5000000: episode: 104, duration: 116.827s, episode steps: 271, steps per second: 2, episode reward: 9.000, mean reward: 0.033 [0.000, 1.000], mean action: 3.771 [0.000, 8.000], mean observation: 87.574 [54.000, 155.000], loss: 0.024688, mean_absolute_error: 0.106349, mean_q: 0.133152, mean_eps: 0.954557\n",
      "   51177/5000000: episode: 105, duration: 236.915s, episode steps: 549, steps per second: 2, episode reward: 33.000, mean reward: 0.060 [0.000, 1.000], mean action: 4.078 [0.000, 8.000], mean observation: 87.268 [54.000, 155.000], loss: 0.026557, mean_absolute_error: 0.106737, mean_q: 0.129727, mean_eps: 0.954188\n",
      "   51640/5000000: episode: 106, duration: 226.522s, episode steps: 463, steps per second: 2, episode reward: 25.000, mean reward: 0.054 [0.000, 1.000], mean action: 4.147 [0.000, 8.000], mean observation: 87.427 [54.000, 155.000], loss: 0.026538, mean_absolute_error: 0.106764, mean_q: 0.128099, mean_eps: 0.953733\n",
      "   52129/5000000: episode: 107, duration: 239.290s, episode steps: 489, steps per second: 2, episode reward: 25.000, mean reward: 0.051 [0.000, 1.000], mean action: 3.988 [0.000, 8.000], mean observation: 87.420 [54.000, 155.000], loss: 0.025051, mean_absolute_error: 0.106422, mean_q: 0.131294, mean_eps: 0.953304\n",
      "   52628/5000000: episode: 108, duration: 231.594s, episode steps: 499, steps per second: 2, episode reward: 26.000, mean reward: 0.052 [0.000, 1.000], mean action: 3.918 [0.000, 8.000], mean observation: 87.386 [54.000, 155.000], loss: 0.025669, mean_absolute_error: 0.106551, mean_q: 0.130492, mean_eps: 0.952860\n",
      "   53097/5000000: episode: 109, duration: 216.203s, episode steps: 469, steps per second: 2, episode reward: 21.000, mean reward: 0.045 [0.000, 1.000], mean action: 4.168 [0.000, 8.000], mean observation: 87.498 [54.000, 155.000], loss: 0.025646, mean_absolute_error: 0.106535, mean_q: 0.132371, mean_eps: 0.952424\n",
      "   53842/5000000: episode: 110, duration: 349.355s, episode steps: 745, steps per second: 2, episode reward: 46.000, mean reward: 0.062 [0.000, 1.000], mean action: 4.165 [0.000, 8.000], mean observation: 87.075 [54.000, 214.000], loss: 0.026194, mean_absolute_error: 0.106669, mean_q: 0.132578, mean_eps: 0.951878\n",
      "   54295/5000000: episode: 111, duration: 210.711s, episode steps: 453, steps per second: 2, episode reward: 15.000, mean reward: 0.033 [0.000, 1.000], mean action: 3.974 [0.000, 8.000], mean observation: 87.532 [54.000, 155.000], loss: 0.025517, mean_absolute_error: 0.106511, mean_q: 0.132944, mean_eps: 0.951339\n",
      "   54948/5000000: episode: 112, duration: 303.265s, episode steps: 653, steps per second: 2, episode reward: 27.000, mean reward: 0.041 [0.000, 1.000], mean action: 4.015 [0.000, 8.000], mean observation: 87.498 [54.000, 155.000], loss: 0.025438, mean_absolute_error: 0.106542, mean_q: 0.135191, mean_eps: 0.950841\n",
      "   55433/5000000: episode: 113, duration: 228.160s, episode steps: 485, steps per second: 2, episode reward: 26.000, mean reward: 0.054 [0.000, 1.000], mean action: 4.054 [0.000, 8.000], mean observation: 87.341 [54.000, 155.000], loss: 0.024736, mean_absolute_error: 0.106313, mean_q: 0.131572, mean_eps: 0.950329\n",
      "   55906/5000000: episode: 114, duration: 225.366s, episode steps: 473, steps per second: 2, episode reward: 24.000, mean reward: 0.051 [0.000, 1.000], mean action: 3.996 [0.000, 8.000], mean observation: 87.474 [54.000, 155.000], loss: 0.025365, mean_absolute_error: 0.106315, mean_q: 0.130959, mean_eps: 0.949898\n",
      "   56471/5000000: episode: 115, duration: 260.402s, episode steps: 565, steps per second: 2, episode reward: 18.000, mean reward: 0.032 [0.000, 1.000], mean action: 4.048 [0.000, 8.000], mean observation: 87.498 [54.000, 155.000], loss: 0.024800, mean_absolute_error: 0.106341, mean_q: 0.133528, mean_eps: 0.949431\n",
      "   57100/5000000: episode: 116, duration: 288.290s, episode steps: 629, steps per second: 2, episode reward: 36.000, mean reward: 0.057 [0.000, 1.000], mean action: 4.135 [0.000, 8.000], mean observation: 87.295 [54.000, 214.000], loss: 0.024096, mean_absolute_error: 0.106212, mean_q: 0.128904, mean_eps: 0.948893\n",
      "   57569/5000000: episode: 117, duration: 210.645s, episode steps: 469, steps per second: 2, episode reward: 14.000, mean reward: 0.030 [0.000, 1.000], mean action: 3.885 [0.000, 8.000], mean observation: 87.592 [54.000, 155.000], loss: 0.026317, mean_absolute_error: 0.106710, mean_q: 0.130587, mean_eps: 0.948399\n",
      "   58076/5000000: episode: 118, duration: 237.049s, episode steps: 507, steps per second: 2, episode reward: 17.000, mean reward: 0.034 [0.000, 1.000], mean action: 4.018 [0.000, 8.000], mean observation: 87.521 [54.000, 155.000], loss: 0.026168, mean_absolute_error: 0.106652, mean_q: 0.134120, mean_eps: 0.947960\n",
      "   58575/5000000: episode: 119, duration: 235.085s, episode steps: 499, steps per second: 2, episode reward: 31.000, mean reward: 0.062 [0.000, 1.000], mean action: 3.954 [0.000, 8.000], mean observation: 87.381 [54.000, 155.000], loss: 0.025722, mean_absolute_error: 0.106589, mean_q: 0.130451, mean_eps: 0.947507\n",
      "   59216/5000000: episode: 120, duration: 317.366s, episode steps: 641, steps per second: 2, episode reward: 36.000, mean reward: 0.056 [0.000, 1.000], mean action: 3.933 [0.000, 8.000], mean observation: 87.268 [54.000, 155.000], loss: 0.025324, mean_absolute_error: 0.106478, mean_q: 0.133814, mean_eps: 0.946995\n",
      "   59535/5000000: episode: 121, duration: 158.378s, episode steps: 319, steps per second: 2, episode reward: 14.000, mean reward: 0.044 [0.000, 1.000], mean action: 4.166 [0.000, 8.000], mean observation: 87.531 [54.000, 155.000], loss: 0.024675, mean_absolute_error: 0.106329, mean_q: 0.132877, mean_eps: 0.946562\n",
      "   59976/5000000: episode: 122, duration: 230.319s, episode steps: 441, steps per second: 2, episode reward: 21.000, mean reward: 0.048 [0.000, 1.000], mean action: 4.054 [0.000, 8.000], mean observation: 87.513 [54.000, 155.000], loss: 0.026125, mean_absolute_error: 0.106677, mean_q: 0.130776, mean_eps: 0.946221\n",
      "   60485/5000000: episode: 123, duration: 261.222s, episode steps: 509, steps per second: 2, episode reward: 29.000, mean reward: 0.057 [0.000, 1.000], mean action: 4.153 [0.000, 8.000], mean observation: 87.276 [54.000, 155.000], loss: 0.024719, mean_absolute_error: 0.105724, mean_q: 0.126046, mean_eps: 0.945793\n",
      "   61100/5000000: episode: 124, duration: 318.678s, episode steps: 615, steps per second: 2, episode reward: 29.000, mean reward: 0.047 [0.000, 1.000], mean action: 3.831 [0.000, 8.000], mean observation: 87.362 [54.000, 155.000], loss: 0.024324, mean_absolute_error: 0.105612, mean_q: 0.126727, mean_eps: 0.945287\n",
      "   61675/5000000: episode: 125, duration: 303.182s, episode steps: 575, steps per second: 2, episode reward: 27.000, mean reward: 0.047 [0.000, 1.000], mean action: 4.003 [0.000, 8.000], mean observation: 87.453 [54.000, 155.000], loss: 0.024209, mean_absolute_error: 0.105675, mean_q: 0.128435, mean_eps: 0.944752\n",
      "   62198/5000000: episode: 126, duration: 270.246s, episode steps: 523, steps per second: 2, episode reward: 24.000, mean reward: 0.046 [0.000, 1.000], mean action: 4.044 [0.000, 8.000], mean observation: 87.521 [54.000, 155.000], loss: 0.024922, mean_absolute_error: 0.105900, mean_q: 0.130478, mean_eps: 0.944258\n",
      "   62623/5000000: episode: 127, duration: 198.060s, episode steps: 425, steps per second: 2, episode reward: 20.000, mean reward: 0.047 [0.000, 1.000], mean action: 4.122 [0.000, 8.000], mean observation: 87.462 [54.000, 155.000], loss: 0.025210, mean_absolute_error: 0.105985, mean_q: 0.130754, mean_eps: 0.943831\n",
      "   63066/5000000: episode: 128, duration: 209.621s, episode steps: 443, steps per second: 2, episode reward: 25.000, mean reward: 0.056 [0.000, 1.000], mean action: 4.097 [0.000, 8.000], mean observation: 87.447 [54.000, 155.000], loss: 0.023746, mean_absolute_error: 0.105641, mean_q: 0.130909, mean_eps: 0.943440\n",
      "   63605/5000000: episode: 129, duration: 262.810s, episode steps: 539, steps per second: 2, episode reward: 26.000, mean reward: 0.048 [0.000, 1.000], mean action: 3.955 [0.000, 8.000], mean observation: 87.453 [54.000, 155.000], loss: 0.024430, mean_absolute_error: 0.105754, mean_q: 0.128605, mean_eps: 0.942998\n",
      "   64086/5000000: episode: 130, duration: 219.735s, episode steps: 481, steps per second: 2, episode reward: 26.000, mean reward: 0.054 [0.000, 1.000], mean action: 3.992 [0.000, 8.000], mean observation: 87.401 [54.000, 155.000], loss: 0.025565, mean_absolute_error: 0.105981, mean_q: 0.130182, mean_eps: 0.942540\n",
      "   64751/5000000: episode: 131, duration: 303.110s, episode steps: 665, steps per second: 2, episode reward: 27.000, mean reward: 0.041 [0.000, 1.000], mean action: 4.113 [0.000, 8.000], mean observation: 87.367 [54.000, 214.000], loss: 0.024191, mean_absolute_error: 0.105781, mean_q: 0.131738, mean_eps: 0.942024\n",
      "   65236/5000000: episode: 132, duration: 235.662s, episode steps: 485, steps per second: 2, episode reward: 28.000, mean reward: 0.058 [0.000, 1.000], mean action: 4.194 [0.000, 8.000], mean observation: 87.386 [54.000, 155.000], loss: 0.026611, mean_absolute_error: 0.106294, mean_q: 0.131576, mean_eps: 0.941506\n",
      "   65579/5000000: episode: 133, duration: 171.820s, episode steps: 343, steps per second: 2, episode reward: 15.000, mean reward: 0.044 [0.000, 1.000], mean action: 4.181 [0.000, 8.000], mean observation: 87.539 [54.000, 155.000], loss: 0.024772, mean_absolute_error: 0.105917, mean_q: 0.134158, mean_eps: 0.941134\n",
      "   66082/5000000: episode: 134, duration: 241.700s, episode steps: 503, steps per second: 2, episode reward: 27.000, mean reward: 0.054 [0.000, 1.000], mean action: 3.855 [0.000, 8.000], mean observation: 87.324 [54.000, 155.000], loss: 0.025385, mean_absolute_error: 0.106032, mean_q: 0.133734, mean_eps: 0.940753\n",
      "   66541/5000000: episode: 135, duration: 219.899s, episode steps: 459, steps per second: 2, episode reward: 23.000, mean reward: 0.050 [0.000, 1.000], mean action: 3.911 [0.000, 8.000], mean observation: 87.374 [54.000, 155.000], loss: 0.024390, mean_absolute_error: 0.105781, mean_q: 0.129970, mean_eps: 0.940320\n",
      "   67076/5000000: episode: 136, duration: 251.375s, episode steps: 535, steps per second: 2, episode reward: 24.000, mean reward: 0.045 [0.000, 1.000], mean action: 3.757 [0.000, 8.000], mean observation: 87.367 [54.000, 155.000], loss: 0.024922, mean_absolute_error: 0.105580, mean_q: 0.127217, mean_eps: 0.939873\n",
      "   67583/5000000: episode: 137, duration: 239.620s, episode steps: 507, steps per second: 2, episode reward: 24.000, mean reward: 0.047 [0.000, 1.000], mean action: 3.819 [0.000, 8.000], mean observation: 87.395 [54.000, 155.000], loss: 0.025279, mean_absolute_error: 0.105838, mean_q: 0.127572, mean_eps: 0.939404\n",
      "   68002/5000000: episode: 138, duration: 191.351s, episode steps: 419, steps per second: 2, episode reward: 25.000, mean reward: 0.060 [0.000, 1.000], mean action: 3.967 [0.000, 8.000], mean observation: 87.417 [54.000, 155.000], loss: 0.024412, mean_absolute_error: 0.105688, mean_q: 0.129115, mean_eps: 0.938987\n",
      "   68475/5000000: episode: 139, duration: 214.403s, episode steps: 473, steps per second: 2, episode reward: 25.000, mean reward: 0.053 [0.000, 1.000], mean action: 3.772 [0.000, 8.000], mean observation: 87.408 [54.000, 155.000], loss: 0.024793, mean_absolute_error: 0.105660, mean_q: 0.127092, mean_eps: 0.938586\n",
      "   68962/5000000: episode: 140, duration: 233.265s, episode steps: 487, steps per second: 2, episode reward: 22.000, mean reward: 0.045 [0.000, 1.000], mean action: 4.127 [0.000, 8.000], mean observation: 87.483 [54.000, 155.000], loss: 0.025731, mean_absolute_error: 0.105960, mean_q: 0.126056, mean_eps: 0.938154\n",
      "   69625/5000000: episode: 141, duration: 317.292s, episode steps: 663, steps per second: 2, episode reward: 42.000, mean reward: 0.063 [0.000, 1.000], mean action: 4.109 [0.000, 8.000], mean observation: 87.344 [54.000, 214.000], loss: 0.026589, mean_absolute_error: 0.106090, mean_q: 0.126928, mean_eps: 0.937636\n",
      "   70124/5000000: episode: 142, duration: 228.576s, episode steps: 499, steps per second: 2, episode reward: 21.000, mean reward: 0.042 [0.000, 1.000], mean action: 3.940 [0.000, 8.000], mean observation: 87.494 [54.000, 155.000], loss: 0.024139, mean_absolute_error: 0.105672, mean_q: 0.128715, mean_eps: 0.937113\n",
      "   70571/5000000: episode: 143, duration: 198.909s, episode steps: 447, steps per second: 2, episode reward: 18.000, mean reward: 0.040 [0.000, 1.000], mean action: 3.957 [0.000, 8.000], mean observation: 87.494 [54.000, 155.000], loss: 0.025077, mean_absolute_error: 0.105926, mean_q: 0.129896, mean_eps: 0.936688\n",
      "   71108/5000000: episode: 144, duration: 255.951s, episode steps: 537, steps per second: 2, episode reward: 31.000, mean reward: 0.058 [0.000, 1.000], mean action: 4.125 [0.000, 8.000], mean observation: 87.430 [54.000, 155.000], loss: 0.025616, mean_absolute_error: 0.106003, mean_q: 0.130146, mean_eps: 0.936245\n",
      "   72087/5000000: episode: 145, duration: 434.986s, episode steps: 979, steps per second: 2, episode reward: 34.000, mean reward: 0.035 [0.000, 1.000], mean action: 3.989 [0.000, 8.000], mean observation: 87.278 [54.000, 214.000], loss: 0.025646, mean_absolute_error: 0.106109, mean_q: 0.133662, mean_eps: 0.935563\n",
      "   72622/5000000: episode: 146, duration: 235.871s, episode steps: 535, steps per second: 2, episode reward: 29.000, mean reward: 0.054 [0.000, 1.000], mean action: 3.935 [0.000, 8.000], mean observation: 87.317 [54.000, 155.000], loss: 0.025845, mean_absolute_error: 0.106132, mean_q: 0.132995, mean_eps: 0.934881\n",
      "   73097/5000000: episode: 147, duration: 281.987s, episode steps: 475, steps per second: 2, episode reward: 27.000, mean reward: 0.057 [0.000, 1.000], mean action: 3.947 [0.000, 8.000], mean observation: 87.386 [54.000, 155.000], loss: 0.025586, mean_absolute_error: 0.105973, mean_q: 0.130001, mean_eps: 0.934427\n",
      "   73502/5000000: episode: 148, duration: 239.296s, episode steps: 405, steps per second: 2, episode reward: 10.000, mean reward: 0.025 [0.000, 1.000], mean action: 3.867 [0.000, 8.000], mean observation: 87.595 [54.000, 155.000], loss: 0.024142, mean_absolute_error: 0.105567, mean_q: 0.129888, mean_eps: 0.934031\n",
      "   73965/5000000: episode: 149, duration: 273.418s, episode steps: 463, steps per second: 2, episode reward: 20.000, mean reward: 0.043 [0.000, 1.000], mean action: 4.123 [0.000, 8.000], mean observation: 87.523 [54.000, 155.000], loss: 0.026455, mean_absolute_error: 0.106229, mean_q: 0.132681, mean_eps: 0.933640\n",
      "   74440/5000000: episode: 150, duration: 279.263s, episode steps: 475, steps per second: 2, episode reward: 27.000, mean reward: 0.057 [0.000, 1.000], mean action: 4.086 [0.000, 8.000], mean observation: 87.427 [54.000, 155.000], loss: 0.023872, mean_absolute_error: 0.105746, mean_q: 0.134422, mean_eps: 0.933218\n",
      "   74837/5000000: episode: 151, duration: 234.333s, episode steps: 397, steps per second: 2, episode reward: 25.000, mean reward: 0.063 [0.000, 1.000], mean action: 3.945 [0.000, 8.000], mean observation: 87.414 [54.000, 155.000], loss: 0.025248, mean_absolute_error: 0.106010, mean_q: 0.133765, mean_eps: 0.932826\n",
      "   75360/5000000: episode: 152, duration: 272.508s, episode steps: 523, steps per second: 2, episode reward: 24.000, mean reward: 0.046 [0.000, 1.000], mean action: 4.094 [0.000, 8.000], mean observation: 87.373 [54.000, 155.000], loss: 0.025421, mean_absolute_error: 0.106034, mean_q: 0.133348, mean_eps: 0.932412\n",
      "   75893/5000000: episode: 153, duration: 227.391s, episode steps: 533, steps per second: 2, episode reward: 22.000, mean reward: 0.041 [0.000, 1.000], mean action: 3.977 [0.000, 8.000], mean observation: 87.525 [54.000, 155.000], loss: 0.024883, mean_absolute_error: 0.105944, mean_q: 0.134291, mean_eps: 0.931937\n",
      "   76398/5000000: episode: 154, duration: 215.918s, episode steps: 505, steps per second: 2, episode reward: 17.000, mean reward: 0.034 [0.000, 1.000], mean action: 3.909 [0.000, 8.000], mean observation: 87.526 [54.000, 155.000], loss: 0.025470, mean_absolute_error: 0.106106, mean_q: 0.136178, mean_eps: 0.931469\n",
      "   76895/5000000: episode: 155, duration: 212.465s, episode steps: 497, steps per second: 2, episode reward: 30.000, mean reward: 0.060 [0.000, 1.000], mean action: 3.932 [0.000, 8.000], mean observation: 87.392 [54.000, 155.000], loss: 0.024301, mean_absolute_error: 0.105883, mean_q: 0.136787, mean_eps: 0.931019\n",
      "   77328/5000000: episode: 156, duration: 184.784s, episode steps: 433, steps per second: 2, episode reward: 17.000, mean reward: 0.039 [0.000, 1.000], mean action: 4.076 [0.000, 8.000], mean observation: 87.560 [54.000, 155.000], loss: 0.025635, mean_absolute_error: 0.106156, mean_q: 0.137085, mean_eps: 0.930600\n",
      "   77879/5000000: episode: 157, duration: 235.447s, episode steps: 551, steps per second: 2, episode reward: 35.000, mean reward: 0.064 [0.000, 1.000], mean action: 4.098 [0.000, 8.000], mean observation: 87.382 [54.000, 155.000], loss: 0.024884, mean_absolute_error: 0.105931, mean_q: 0.133741, mean_eps: 0.930157\n",
      "   78352/5000000: episode: 158, duration: 202.042s, episode steps: 473, steps per second: 2, episode reward: 17.000, mean reward: 0.036 [0.000, 1.000], mean action: 3.992 [0.000, 8.000], mean observation: 87.519 [54.000, 155.000], loss: 0.025266, mean_absolute_error: 0.105906, mean_q: 0.129531, mean_eps: 0.929697\n",
      "   78833/5000000: episode: 159, duration: 205.640s, episode steps: 481, steps per second: 2, episode reward: 18.000, mean reward: 0.037 [0.000, 1.000], mean action: 3.902 [0.000, 8.000], mean observation: 87.505 [54.000, 155.000], loss: 0.023840, mean_absolute_error: 0.105609, mean_q: 0.130267, mean_eps: 0.929267\n",
      "   79220/5000000: episode: 160, duration: 165.412s, episode steps: 387, steps per second: 2, episode reward: 19.000, mean reward: 0.049 [0.000, 1.000], mean action: 3.837 [0.000, 8.000], mean observation: 87.519 [54.000, 155.000], loss: 0.024988, mean_absolute_error: 0.105882, mean_q: 0.130468, mean_eps: 0.928877\n",
      "   79827/5000000: episode: 161, duration: 259.561s, episode steps: 607, steps per second: 2, episode reward: 29.000, mean reward: 0.048 [0.000, 1.000], mean action: 4.115 [0.000, 8.000], mean observation: 87.370 [54.000, 155.000], loss: 0.024448, mean_absolute_error: 0.105855, mean_q: 0.134952, mean_eps: 0.928429\n",
      "   80346/5000000: episode: 162, duration: 221.895s, episode steps: 519, steps per second: 2, episode reward: 29.000, mean reward: 0.056 [0.000, 1.000], mean action: 4.091 [0.000, 8.000], mean observation: 87.409 [54.000, 155.000], loss: 0.025630, mean_absolute_error: 0.106450, mean_q: 0.135760, mean_eps: 0.927923\n",
      "   80859/5000000: episode: 163, duration: 219.474s, episode steps: 513, steps per second: 2, episode reward: 22.000, mean reward: 0.043 [0.000, 1.000], mean action: 3.897 [0.000, 8.000], mean observation: 87.390 [54.000, 155.000], loss: 0.025795, mean_absolute_error: 0.106699, mean_q: 0.137747, mean_eps: 0.927458\n",
      "   81242/5000000: episode: 164, duration: 164.306s, episode steps: 383, steps per second: 2, episode reward: 18.000, mean reward: 0.047 [0.000, 1.000], mean action: 3.990 [0.000, 8.000], mean observation: 87.507 [54.000, 155.000], loss: 0.025123, mean_absolute_error: 0.106603, mean_q: 0.141476, mean_eps: 0.927055\n",
      "   81779/5000000: episode: 165, duration: 229.962s, episode steps: 537, steps per second: 2, episode reward: 22.000, mean reward: 0.041 [0.000, 1.000], mean action: 4.041 [0.000, 8.000], mean observation: 87.499 [54.000, 155.000], loss: 0.024594, mean_absolute_error: 0.106433, mean_q: 0.136871, mean_eps: 0.926641\n",
      "   82268/5000000: episode: 166, duration: 209.490s, episode steps: 489, steps per second: 2, episode reward: 19.000, mean reward: 0.039 [0.000, 1.000], mean action: 4.063 [0.000, 8.000], mean observation: 87.505 [54.000, 155.000], loss: 0.023893, mean_absolute_error: 0.106226, mean_q: 0.135073, mean_eps: 0.926179\n",
      "   82673/5000000: episode: 167, duration: 173.230s, episode steps: 405, steps per second: 2, episode reward: 18.000, mean reward: 0.044 [0.000, 1.000], mean action: 4.114 [0.000, 8.000], mean observation: 87.497 [54.000, 155.000], loss: 0.025664, mean_absolute_error: 0.106641, mean_q: 0.135117, mean_eps: 0.925777\n",
      "   83196/5000000: episode: 168, duration: 224.097s, episode steps: 523, steps per second: 2, episode reward: 21.000, mean reward: 0.040 [0.000, 1.000], mean action: 3.841 [0.000, 8.000], mean observation: 87.422 [54.000, 155.000], loss: 0.026152, mean_absolute_error: 0.106800, mean_q: 0.137918, mean_eps: 0.925359\n",
      "   83669/5000000: episode: 169, duration: 202.596s, episode steps: 473, steps per second: 2, episode reward: 17.000, mean reward: 0.036 [0.000, 1.000], mean action: 4.203 [0.000, 8.000], mean observation: 87.492 [54.000, 155.000], loss: 0.025595, mean_absolute_error: 0.106649, mean_q: 0.137405, mean_eps: 0.924911\n",
      "   84170/5000000: episode: 170, duration: 214.892s, episode steps: 501, steps per second: 2, episode reward: 19.000, mean reward: 0.038 [0.000, 1.000], mean action: 3.836 [0.000, 8.000], mean observation: 87.515 [54.000, 155.000], loss: 0.024863, mean_absolute_error: 0.106492, mean_q: 0.137144, mean_eps: 0.924473\n",
      "   84733/5000000: episode: 171, duration: 246.051s, episode steps: 563, steps per second: 2, episode reward: 32.000, mean reward: 0.057 [0.000, 1.000], mean action: 3.897 [0.000, 8.000], mean observation: 87.368 [54.000, 155.000], loss: 0.025193, mean_absolute_error: 0.106570, mean_q: 0.137445, mean_eps: 0.923994\n",
      "   85208/5000000: episode: 172, duration: 208.354s, episode steps: 475, steps per second: 2, episode reward: 20.000, mean reward: 0.042 [0.000, 1.000], mean action: 4.040 [0.000, 8.000], mean observation: 87.475 [54.000, 155.000], loss: 0.025582, mean_absolute_error: 0.106679, mean_q: 0.138561, mean_eps: 0.923527\n",
      "   85597/5000000: episode: 173, duration: 168.394s, episode steps: 389, steps per second: 2, episode reward: 19.000, mean reward: 0.049 [0.000, 1.000], mean action: 4.015 [0.000, 8.000], mean observation: 87.504 [54.000, 155.000], loss: 0.026227, mean_absolute_error: 0.106821, mean_q: 0.139106, mean_eps: 0.923138\n",
      "   86102/5000000: episode: 174, duration: 218.420s, episode steps: 505, steps per second: 2, episode reward: 32.000, mean reward: 0.063 [0.000, 1.000], mean action: 4.004 [0.000, 8.000], mean observation: 87.379 [54.000, 155.000], loss: 0.026518, mean_absolute_error: 0.106787, mean_q: 0.134886, mean_eps: 0.922736\n",
      "   86687/5000000: episode: 175, duration: 252.997s, episode steps: 585, steps per second: 2, episode reward: 24.000, mean reward: 0.041 [0.000, 1.000], mean action: 3.933 [0.000, 8.000], mean observation: 87.428 [54.000, 155.000], loss: 0.026313, mean_absolute_error: 0.106783, mean_q: 0.134689, mean_eps: 0.922245\n",
      "   87418/5000000: episode: 176, duration: 315.906s, episode steps: 731, steps per second: 2, episode reward: 36.000, mean reward: 0.049 [0.000, 1.000], mean action: 4.051 [0.000, 8.000], mean observation: 87.244 [54.000, 214.000], loss: 0.025343, mean_absolute_error: 0.106607, mean_q: 0.136790, mean_eps: 0.921653\n",
      "   87907/5000000: episode: 177, duration: 211.586s, episode steps: 489, steps per second: 2, episode reward: 25.000, mean reward: 0.051 [0.000, 1.000], mean action: 3.855 [0.000, 8.000], mean observation: 87.484 [54.000, 155.000], loss: 0.023281, mean_absolute_error: 0.106166, mean_q: 0.136816, mean_eps: 0.921104\n",
      "   88388/5000000: episode: 178, duration: 207.528s, episode steps: 481, steps per second: 2, episode reward: 22.000, mean reward: 0.046 [0.000, 1.000], mean action: 4.135 [0.000, 8.000], mean observation: 87.501 [54.000, 155.000], loss: 0.025829, mean_absolute_error: 0.106649, mean_q: 0.135961, mean_eps: 0.920668\n",
      "   88923/5000000: episode: 179, duration: 230.758s, episode steps: 535, steps per second: 2, episode reward: 35.000, mean reward: 0.065 [0.000, 1.000], mean action: 3.918 [0.000, 8.000], mean observation: 87.260 [54.000, 155.000], loss: 0.023338, mean_absolute_error: 0.106150, mean_q: 0.135778, mean_eps: 0.920210\n",
      "   89348/5000000: episode: 180, duration: 185.018s, episode steps: 425, steps per second: 2, episode reward: 19.000, mean reward: 0.045 [0.000, 1.000], mean action: 4.235 [0.000, 8.000], mean observation: 87.525 [54.000, 155.000], loss: 0.025921, mean_absolute_error: 0.106681, mean_q: 0.136463, mean_eps: 0.919779\n",
      "   89797/5000000: episode: 181, duration: 198.206s, episode steps: 449, steps per second: 2, episode reward: 16.000, mean reward: 0.036 [0.000, 1.000], mean action: 3.915 [0.000, 8.000], mean observation: 87.540 [54.000, 155.000], loss: 0.024933, mean_absolute_error: 0.106468, mean_q: 0.133528, mean_eps: 0.919385\n",
      "   90294/5000000: episode: 182, duration: 218.965s, episode steps: 497, steps per second: 2, episode reward: 22.000, mean reward: 0.044 [0.000, 1.000], mean action: 3.915 [0.000, 8.000], mean observation: 87.479 [54.000, 155.000], loss: 0.025707, mean_absolute_error: 0.106527, mean_q: 0.134479, mean_eps: 0.918960\n",
      "   90699/5000000: episode: 183, duration: 174.845s, episode steps: 405, steps per second: 2, episode reward: 19.000, mean reward: 0.047 [0.000, 1.000], mean action: 3.980 [0.000, 8.000], mean observation: 87.472 [54.000, 155.000], loss: 0.025858, mean_absolute_error: 0.106519, mean_q: 0.135539, mean_eps: 0.918554\n",
      "   91118/5000000: episode: 184, duration: 180.444s, episode steps: 419, steps per second: 2, episode reward: 22.000, mean reward: 0.053 [0.000, 1.000], mean action: 3.866 [0.000, 8.000], mean observation: 87.412 [54.000, 155.000], loss: 0.025255, mean_absolute_error: 0.106382, mean_q: 0.136446, mean_eps: 0.918183\n",
      "   91609/5000000: episode: 185, duration: 211.435s, episode steps: 491, steps per second: 2, episode reward: 26.000, mean reward: 0.053 [0.000, 1.000], mean action: 3.902 [0.000, 8.000], mean observation: 87.440 [54.000, 155.000], loss: 0.024738, mean_absolute_error: 0.106255, mean_q: 0.135108, mean_eps: 0.917773\n",
      "   92280/5000000: episode: 186, duration: 291.842s, episode steps: 671, steps per second: 2, episode reward: 42.000, mean reward: 0.063 [0.000, 1.000], mean action: 4.007 [0.000, 8.000], mean observation: 87.185 [54.000, 214.000], loss: 0.025613, mean_absolute_error: 0.106420, mean_q: 0.135289, mean_eps: 0.917250\n",
      "   92607/5000000: episode: 187, duration: 143.503s, episode steps: 327, steps per second: 2, episode reward: 16.000, mean reward: 0.049 [0.000, 1.000], mean action: 3.771 [0.000, 8.000], mean observation: 87.508 [54.000, 155.000], loss: 0.025361, mean_absolute_error: 0.106413, mean_q: 0.134632, mean_eps: 0.916801\n",
      "   93052/5000000: episode: 188, duration: 195.477s, episode steps: 445, steps per second: 2, episode reward: 23.000, mean reward: 0.052 [0.000, 1.000], mean action: 3.881 [0.000, 8.000], mean observation: 87.482 [54.000, 155.000], loss: 0.024536, mean_absolute_error: 0.106163, mean_q: 0.131864, mean_eps: 0.916454\n",
      "   93515/5000000: episode: 189, duration: 203.156s, episode steps: 463, steps per second: 2, episode reward: 23.000, mean reward: 0.050 [0.000, 1.000], mean action: 4.015 [0.000, 8.000], mean observation: 87.435 [54.000, 155.000], loss: 0.025760, mean_absolute_error: 0.106491, mean_q: 0.135657, mean_eps: 0.916045\n",
      "   93990/5000000: episode: 190, duration: 208.702s, episode steps: 475, steps per second: 2, episode reward: 19.000, mean reward: 0.040 [0.000, 1.000], mean action: 4.137 [0.000, 8.000], mean observation: 87.476 [54.000, 155.000], loss: 0.025697, mean_absolute_error: 0.106517, mean_q: 0.138539, mean_eps: 0.915623\n",
      "   94597/5000000: episode: 191, duration: 265.214s, episode steps: 607, steps per second: 2, episode reward: 37.000, mean reward: 0.061 [0.000, 1.000], mean action: 3.992 [0.000, 8.000], mean observation: 87.260 [54.000, 214.000], loss: 0.025106, mean_absolute_error: 0.106327, mean_q: 0.136847, mean_eps: 0.915136\n",
      "   95072/5000000: episode: 192, duration: 209.598s, episode steps: 475, steps per second: 2, episode reward: 26.000, mean reward: 0.055 [0.000, 1.000], mean action: 3.819 [0.000, 8.000], mean observation: 87.497 [54.000, 155.000], loss: 0.026375, mean_absolute_error: 0.106687, mean_q: 0.139832, mean_eps: 0.914649\n",
      "   95647/5000000: episode: 193, duration: 250.960s, episode steps: 575, steps per second: 2, episode reward: 27.000, mean reward: 0.047 [0.000, 1.000], mean action: 3.911 [0.000, 8.000], mean observation: 87.462 [54.000, 155.000], loss: 0.026939, mean_absolute_error: 0.106830, mean_q: 0.140552, mean_eps: 0.914177\n",
      "   96122/5000000: episode: 194, duration: 208.705s, episode steps: 475, steps per second: 2, episode reward: 33.000, mean reward: 0.069 [0.000, 1.000], mean action: 3.882 [0.000, 8.000], mean observation: 87.349 [54.000, 155.000], loss: 0.025260, mean_absolute_error: 0.106514, mean_q: 0.141497, mean_eps: 0.913704\n",
      "   96621/5000000: episode: 195, duration: 218.580s, episode steps: 499, steps per second: 2, episode reward: 19.000, mean reward: 0.038 [0.000, 1.000], mean action: 3.776 [0.000, 8.000], mean observation: 87.535 [54.000, 155.000], loss: 0.025552, mean_absolute_error: 0.106530, mean_q: 0.140999, mean_eps: 0.913266\n",
      "   97064/5000000: episode: 196, duration: 195.089s, episode steps: 443, steps per second: 2, episode reward: 17.000, mean reward: 0.038 [0.000, 1.000], mean action: 3.892 [0.000, 8.000], mean observation: 87.536 [54.000, 155.000], loss: 0.024901, mean_absolute_error: 0.106484, mean_q: 0.143158, mean_eps: 0.912842\n",
      "   97587/5000000: episode: 197, duration: 230.339s, episode steps: 523, steps per second: 2, episode reward: 22.000, mean reward: 0.042 [0.000, 1.000], mean action: 4.141 [0.000, 8.000], mean observation: 87.420 [54.000, 155.000], loss: 0.026155, mean_absolute_error: 0.106744, mean_q: 0.142373, mean_eps: 0.912408\n",
      "   98032/5000000: episode: 198, duration: 14822.297s, episode steps: 445, steps per second: 0, episode reward: 20.000, mean reward: 0.045 [0.000, 1.000], mean action: 3.903 [0.000, 8.000], mean observation: 87.469 [54.000, 155.000], loss: 0.024820, mean_absolute_error: 0.106399, mean_q: 0.142527, mean_eps: 0.911972\n",
      "   98845/5000000: episode: 199, duration: 403.047s, episode steps: 813, steps per second: 2, episode reward: 52.000, mean reward: 0.064 [0.000, 1.000], mean action: 3.978 [0.000, 8.000], mean observation: 86.916 [54.000, 214.000], loss: 0.024917, mean_absolute_error: 0.106393, mean_q: 0.141214, mean_eps: 0.911406\n",
      "done, took 60477.570 seconds\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(initial_seed)\n",
    "env.seed(initial_seed)\n",
    "a.__train__(env, steps=5000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 7.000, steps: 443\n",
      "Episode 2: reward: 7.000, steps: 443\n",
      "Episode 3: reward: 7.000, steps: 443\n",
      "Episode 4: reward: 7.000, steps: 443\n",
      "Episode 5: reward: 7.000, steps: 443\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(initial_seed)\n",
    "env.seed(initial_seed)\n",
    "a.__test__(env, episodes=5, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward 240.0\n"
     ]
    }
   ],
   "source": [
    "a.__replay__(env, condition=(lambda info : info is None or info['ale.lives'] > 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
